from typing import Any, List, Optional, Callable
from config import config

from llama_index.core.agent.react.types import (
    ActionReasoningStep,
    ObservationReasoningStep,
)
from llama_index.core.llms.llm import LLM
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.tools.types import BaseTool
from llama_index.core.workflow import (
    Context,
    Workflow,
    StartEvent,
    StopEvent,
    step,
)

from workflow_templates import get_workflow_template_by_device_type
from ReAct_Events import *
from ReAct_Tools import *
from llama_index.core.base.llms.types import ChatMessage, MessageRole
from custom_react_system_prompt import (CUSTOM_REACT_CHAT_SYSTEM_HEADER,
                                        CUSTOM_CONTEXT_REACT_CHAT_SYSTEM_HEADER,
                                        CONCLUSION_PROMPT_TEMPLATE,
                                        FILTER_PROMPT_TEMPLATE,
                                        PLANNING_JUDGE_TEMPLATE,
                                        PLANNING_TEMPLATE,
                                        PLAN_MODIFY_TEMPLATE,
                                        PLAN_UPDATE_TEMPLATE,
                                        ENTITY_RECOGNITION_TEMPLATE,
                                        INTENT_RECOGNITION_TEMPLATE,
                                        )
from custom_formatter import (customReActChatFormatter,
                              ConclusionChatFormatter,
                              FilterChatFormatter,
                              PlanningJudgeFormatter,
                              PlanningFormatter,
                              PlanModifyFormatter,
                              PlanUpdateFormatter,
                              EntityRecognitionFormatter,
                              IntentRecognitionFormatter,
                              )
from custom_output_parser import customReActOutputParser
from custom_reasoning_step import MilestoneReasoningStep
from custom_dashscope_llm import customDashscopeLLM
from fast_mcp_client import MCPClient
from workflow_strategy import WorkflowStrategy, DefaultWorkflowStrategy
from streaming_response_parser import StreamingResponseParser
from category_llm_manager import get_global_llm_manager
from category_workflow_context import get_global_context_manager
import json, re, traceback
import asyncio
import concurrent.futures
from typing import List, Dict, Any
import copy
from dataclasses import dataclass


@dataclass
class WorkflowResult:
    """å·¥ä½œæµç¨‹æ‰§è¡Œç»“æœ"""
    category: str
    status: str  # "completed", "failed", "timeout"
    reasoning: List[Any] = None
    sources: List[Any] = None
    error: str = None
    execution_time: float = 0.0
    response: str | None = None


class ParallelWorkflowManager:
    """å¢å¼ºçš„å¹¶è¡Œå·¥ä½œæµç¨‹ç®¡ç†å™¨ï¼Œæ”¯æŒç‹¬ç«‹Agentå®ä¾‹"""
    
    def __init__(self, agent_instance):
        self.base_agent = agent_instance
        self.workflow_tasks = {}  # {workflow_category: asyncio.Task}
        self.workflow_results = {}  # {workflow_category: WorkflowResult}
        self.workflow_contexts = {}  # {workflow_category: Context}
        self.category_agents = {}  # {workflow_category: ReActAgent}
        self.llm_manager = get_global_llm_manager()
        self.context_manager = get_global_context_manager()
    
    def get_or_create_category_agent(self, category: str):
        """è·å–æˆ–åˆ›å»ºæŒ‡å®šåˆ†ç±»çš„ç‹¬ç«‹Agentå®ä¾‹"""
        if category not in self.category_agents:
            # è·å–è¯¥åˆ†ç±»çš„ç‹¬ç«‹LLMå®ä¾‹
            category_llms = self.llm_manager.create_full_llm_set(category)
            
            # åˆ›å»ºç‹¬ç«‹çš„Agentå®ä¾‹ï¼Œå…±äº«MCPå®¢æˆ·ç«¯
            category_agent = ReActAgent(
                llm=category_llms['main_llm'],
                tools=self.base_agent.tools,
                workflow_strategy=self.base_agent.workflow_strategy,
                workflow_category=category
            )
            
            # å…³é”®ä¿®æ”¹ï¼šå…±äº«åŸºç¡€Agentçš„MCPå®¢æˆ·ç«¯å®ä¾‹ï¼Œé¿å…é‡å¤è¿æ¥
            if hasattr(self.base_agent, 'mcp_client'):
                category_agent.mcp_client = self.base_agent.mcp_client
            
            # è®¾ç½®åˆ†ç±»ç‰¹å®šçš„LLMå®ä¾‹
            category_agent.conclusion_llm = category_llms['conclusion_llm']
            category_agent.filter_llm = category_llms['filter_llm']
            category_agent.planning_llm = category_llms['planning_llm']
            category_agent.entity_recognition_llm = category_llms['entity_recognition_llm']
            category_agent.intent_recognition_llm = category_llms['intent_recognition_llm']
            # è®¾ç½®å…¶ä»–å¯èƒ½ç¼ºå¤±çš„LLMå®ä¾‹ï¼Œé¿å…NoneTypeé”™è¯¯
            category_agent.planning_judge_llm = category_llms.get('planning_judge_llm', category_llms['main_llm'])
            category_agent.plan_modify_llm = category_llms.get('plan_modify_llm', category_llms['main_llm'])
            category_agent.plan_update_llm = category_llms.get('plan_update_llm', category_llms['main_llm'])
            
            self.category_agents[category] = category_agent
            print(f"âœ… ä¸ºåˆ†ç±» '{category}' åˆ›å»ºç‹¬ç«‹Agentå®ä¾‹ (å…±äº«MCPå®¢æˆ·ç«¯)")
        
        return self.category_agents[category]
        
    async def create_workflow_context(self, base_ctx: Context, category: str) -> Context:
        """ä¸ºæ¯ä¸ªå·¥ä½œæµç¨‹åˆ›å»ºç‹¬ç«‹çš„ä¸Šä¸‹æ–‡"""
        # è·å–è¯¥åˆ†ç±»çš„ç‹¬ç«‹Agentå®ä¾‹
        category_agent = self.get_or_create_category_agent(category)
        
        # åˆ›å»ºæ–°çš„ä¸Šä¸‹æ–‡å®ä¾‹ï¼Œä½¿ç”¨åˆ†ç±»ç‰¹å®šçš„ä»£ç†å®ä¾‹
        workflow_ctx = Context(workflow=category_agent)
        
        # å¤åˆ¶åŸºç¡€æ•°æ®
        base_data = {
            "recognized_entities": await base_ctx.store.get("recognized_entities"),
            "user_input": await base_ctx.store.get("user_input"),
            "memory": await base_ctx.store.get("memory"),
            "input_metadata": await base_ctx.store.get("input_metadata"),
            "workflow_categories": await base_ctx.store.get("workflow_categories", default=[]),
        }
        
        for key, value in base_data.items():
            if value is not None:
                # æ·±æ‹·è´ä»¥é¿å…ä¸Šä¸‹æ–‡é—´çš„æ•°æ®æ±¡æŸ“
                copied_value = copy.deepcopy(value) if isinstance(value, (list, dict)) else value
                await workflow_ctx.store.set(key, copied_value)
        
        # è®¾ç½®å·¥ä½œæµç¨‹ç‰¹å®šçš„æ ‡è¯†
        await workflow_ctx.store.set("current_workflow_category", category)
        await workflow_ctx.store.set("workflow_id", f"{category}_{id(workflow_ctx)}")
        await workflow_ctx.store.set("current_reasoning", [])
        await workflow_ctx.store.set("sources", [])
        
        # ä½¿ç”¨åˆ†ç±»ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–ä¸Šä¸‹æ–‡
        category_context = self.context_manager.get_category_context(category, workflow_ctx)
        # è®¾ç½®åˆ†ç±»ç‰¹å®šçš„åˆå§‹åŒ–æ•°æ®
        await category_context.set("workflow_initialized", True)
        await category_context.set("workflow_start_time", asyncio.get_event_loop().time())
        
        return workflow_ctx
    
    async def execute_single_workflow(self, workflow_ctx: Context, ev: EntityAnalysisEvent, category: str) -> WorkflowResult:
        """æ‰§è¡Œå•ä¸ªå·¥ä½œæµç¨‹"""
        import time
        start_time = time.time()
        
        try:
            print(f"\nğŸš€ å¯åŠ¨å¹¶è¡Œå·¥ä½œæµç¨‹: {category}")
            
            # è·å–åˆ†ç±»ç‰¹å®šçš„Agentå®ä¾‹
            category_agent = self.get_or_create_category_agent(category)
            
            # é€‰æ‹©å·¥ä½œæµæ¨¡æ¿
            try:
                selected_template = category_agent._select_workflow_template(ev.recognized_entities, category)
                await workflow_ctx.store.set("selected_workflow_template", selected_template)
                print(f"ğŸ“„ {category} é€‰æ‹©çš„å·¥ä½œæµæ¨¡æ¿: {selected_template[:50]}...")
            except Exception as e:
                print(f"âš ï¸ {category} é€‰æ‹©æ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {e}")
                await workflow_ctx.store.set("selected_workflow_template", "é€šç”¨æ•…éšœå¤„ç†æµç¨‹")
            
            # æ‰§è¡Œå·¥ä½œæµç¨‹çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ
            # å¢åŠ è¶…æ—¶æ§åˆ¶ï¼Œé¿å…å•ä¸ªå·¥ä½œæµå¡æ­»æ•´ä¸ªå¹¶è¡Œç»„
            # æ³¨æ„ï¼šè¿™é‡Œçš„timeoutåº”è¯¥æ¯”æ€»çš„å¹¶è¡Œè¶…æ—¶æ—¶é—´çŸ­ä¸€äº›ï¼Œæˆ–è€…ç”±å¤–å±‚æ§åˆ¶
            # è¿™é‡Œæˆ‘ä»¬ä¸åŠ wait_forï¼Œè®©å¤–å±‚çš„gatherç»Ÿä¸€æ§åˆ¶è¶…æ—¶
            result = await self._run_workflow_lifecycle(workflow_ctx, ev, category)
            
            execution_time = time.time() - start_time
            print(f"âœ… {category} å·¥ä½œæµç¨‹å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
            
            return WorkflowResult(
                category=category,
                status="completed",
                reasoning=result.get("reasoning", []),
                sources=result.get("sources", []),
                response=result.get("response"),
                execution_time=execution_time
            )
            
        except asyncio.TimeoutError:
            execution_time = time.time() - start_time
            print(f"â° {category} å·¥ä½œæµç¨‹è¶…æ—¶ï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
            return WorkflowResult(
                category=category,
                status="timeout",
                error="å·¥ä½œæµç¨‹æ‰§è¡Œè¶…æ—¶",
                execution_time=execution_time
            )
        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âŒ {category} å·¥ä½œæµç¨‹å¤±è´¥: {str(e)}")
            return WorkflowResult(
                category=category,
                status="failed",
                error=str(e),
                execution_time=execution_time
            )
    
    async def _run_workflow_lifecycle(self, workflow_ctx: Context, ev: EntityAnalysisEvent, category: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªå·¥ä½œæµç¨‹çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ"""
        try:
            # è·å–åˆ†ç±»ç‰¹å®šçš„Agentå®ä¾‹
            category_agent = self.get_or_create_category_agent(category)
            
            # 1. ç”Ÿæˆè®¡åˆ’
            user_input = await workflow_ctx.store.get("user_input")
            planning_event = PlanningEvent(input=user_input, additional_input=[])
            
            # è°ƒç”¨åˆ†ç±»ç‰¹å®šagentçš„generate_planæ–¹æ³•
            input_event = await category_agent.generate_plan(workflow_ctx, planning_event)
            
            # 2. å‡†å¤‡èŠå¤©å†å²
            prep_event = PrepEvent(input=user_input)
            input_event = await category_agent.prepare_chat_history(workflow_ctx, prep_event)
            
            # 3. å¤„ç†LLMè¾“å…¥ï¼Œä½¿ç”¨åŠ¨æ€ç»ˆæ­¢æ¡ä»¶
            current_reasoning = []
            sources = []
            iteration = 0
            final_response = None
            while True:
                try:
                    # å¤„ç†LLMè¾“å…¥
                    result = await category_agent.handle_llm_input(workflow_ctx, input_event)
                    
                    if isinstance(result, StopEvent):
                        # å·¥ä½œæµç¨‹å®Œæˆï¼Œæå–æœ€ç»ˆå“åº”
                        try:
                            res_obj = getattr(result, "result", {}) or {}
                            final_response = res_obj.get("response")
                        except Exception:
                            final_response = None
                        break
                    elif isinstance(result, ToolCallEvent):
                        # å¤„ç†å·¥å…·è°ƒç”¨
                        prep_event = await category_agent.handle_tool_calls(workflow_ctx, result)
                        input_event = await category_agent.prepare_chat_history(workflow_ctx, prep_event)
                        
                        # æ”¶é›†æ¨ç†æ­¥éª¤
                        reasoning_steps = await workflow_ctx.store.get("current_reasoning", default=[])
                        if reasoning_steps:
                            current_reasoning.extend(reasoning_steps)
                        
                        # æ”¶é›†æ•°æ®æº
                        current_sources = await workflow_ctx.store.get("sources", default=[])
                        if current_sources:
                            sources.extend(current_sources)
                    iteration += 1
                
                except Exception as e:
                    print(f"âš ï¸ {category} å·¥ä½œæµç¨‹ç¬¬ {iteration} æ¬¡è¿­ä»£å‡ºé”™: {str(e)}")
                    break
            
            # è·å–æœ€ç»ˆç»“æœ
            final_reasoning = await workflow_ctx.store.get("current_reasoning", default=current_reasoning)
            final_sources = await workflow_ctx.store.get("sources", default=sources)
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°ç»“æœï¼Œæä¾›é»˜è®¤ç»“æœ
            if not final_reasoning:
                final_reasoning = [f"{category} å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆ"]
            if not final_sources:
                final_sources = [f"{category} ç›¸å…³æ•°æ®æº"]
            
            return {
                "reasoning": final_reasoning,
                "sources": final_sources,
                "response": final_response,
            }
            
        except Exception as e:
            print(f"âŒ {category} å·¥ä½œæµç¨‹ç”Ÿå‘½å‘¨æœŸæ‰§è¡Œå¤±è´¥: {str(e)}")
            # è¿”å›é”™è¯¯ä¿¡æ¯ä½œä¸ºç»“æœ
            return {
                "reasoning": [f"{category} å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}"],
                "sources": []
            }
    
    async def execute_parallel_workflows(
        self, 
        base_ctx: Context, 
        ev: EntityAnalysisEvent, 
        categories: List[str], 
        timeout: float = 30.0,
        on_thinking: Optional[Callable] = None,
        on_content: Optional[Callable] = None
    ) -> Dict[str, WorkflowResult]:
        """
        å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå·¥ä½œæµç¨‹åˆ†ç±»
        
        Args:
            base_ctx: åŸºç¡€ä¸Šä¸‹æ–‡
            ev: å®ä½“åˆ†æäº‹ä»¶
            categories: å·¥ä½œæµç¨‹åˆ†ç±»åˆ—è¡¨
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            on_thinking: æ€è€ƒè¿‡ç¨‹å›è°ƒå‡½æ•°
            on_content: å†…å®¹å›è°ƒå‡½æ•°
        
        Returns:
            Dict[str, WorkflowResult]: å„åˆ†ç±»çš„å·¥ä½œæµç¨‹ç»“æœ
        """
        print(f"\nğŸ”„ å¼€å§‹å¹¶è¡Œæ‰§è¡Œå·¥ä½œæµç¨‹ï¼Œåˆ†ç±»: {categories}")
        
        # è·å–åŸºç¡€Agentçš„æµå¼å“åº”è§£æå™¨
        response_parser = self.base_agent.response_parser
        
        # ä¸ºæ¯ä¸ªåˆ†ç±»åˆ›å»ºç‹¬ç«‹çš„ä¸Šä¸‹æ–‡å’Œä»»åŠ¡
        tasks = {}
        streaming_responses = {}
        
        for category in categories:
            try:
                # åˆ›å»ºåˆ†ç±»ç‰¹å®šçš„ä¸Šä¸‹æ–‡
                workflow_ctx = await self.create_workflow_context(base_ctx, category)
                self.workflow_contexts[category] = workflow_ctx
                
                # è·å–åˆ†ç±»ç‰¹å®šçš„Agent
                category_agent = self.get_or_create_category_agent(category)
                
                # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
                task = asyncio.create_task(
                    self.execute_single_workflow(workflow_ctx, ev, category)
                )
                tasks[category] = task
                self.workflow_tasks[category] = task
                
                # å­˜å‚¨æµå¼å“åº”
                if hasattr(category_agent, 'streaming_response'):
                    streaming_responses[category] = category_agent.streaming_response
                
                print(f"ğŸ“‹ ä¸ºåˆ†ç±» '{category}' åˆ›å»ºæ‰§è¡Œä»»åŠ¡")
                
            except Exception as e:
                print(f"âŒ åˆ›å»ºåˆ†ç±» '{category}' çš„ä»»åŠ¡å¤±è´¥: {str(e)}")
                self.workflow_results[category] = WorkflowResult(
                    category=category,
                    status="failed",
                    error=f"ä»»åŠ¡åˆ›å»ºå¤±è´¥: {str(e)}"
                )
        
        # å¤„ç†æµå¼å“åº”
        if streaming_responses and on_thinking and on_content:
            try:
                # åˆ›å»ºæµå¼å“åº”è§£æä»»åŠ¡
                streaming_task = asyncio.create_task(
                    response_parser.parse_parallel_streaming_response(
                        streaming_responses,
                        on_thinking,
                        on_content
                    )
                )
            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºæµå¼å“åº”è§£æä»»åŠ¡å¤±è´¥: {str(e)}")
                streaming_task = None
        else:
            streaming_task = None
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆæˆ–è¶…æ—¶
        if tasks:
            try:
                print(f"â±ï¸ ç­‰å¾…æ‰€æœ‰å·¥ä½œæµç¨‹å®Œæˆï¼Œè¶…æ—¶æ—¶é—´: {timeout}ç§’")
                # ä½¿ç”¨shieldé˜²æ­¢å¤–éƒ¨å–æ¶ˆå½±å“å†…éƒ¨ä»»åŠ¡ï¼ŒåŒæ—¶ç¡®ä¿wait_forç”Ÿæ•ˆ
                results = await asyncio.wait_for(
                    asyncio.gather(*tasks.values(), return_exceptions=True),
                    timeout=timeout
                )
                
                # å¤„ç†ç»“æœ
                for i, (category, task) in enumerate(tasks.items()):
                    if i < len(results):
                        result = results[i]
                        if isinstance(result, Exception):
                            # åŒºåˆ†å–æ¶ˆå¼‚å¸¸å’Œå…¶ä»–å¼‚å¸¸
                            if isinstance(result, asyncio.CancelledError):
                                print(f"âš ï¸ åˆ†ç±» '{category}' ä»»åŠ¡è¢«å–æ¶ˆ")
                                self.workflow_results[category] = WorkflowResult(
                                    category=category,
                                    status="cancelled",
                                    error="ä»»åŠ¡è¢«å–æ¶ˆ"
                                )
                            else:
                                print(f"âŒ åˆ†ç±» '{category}' æ‰§è¡Œå¼‚å¸¸: {str(result)}")
                                self.workflow_results[category] = WorkflowResult(
                                    category=category,
                                    status="failed",
                                    error=str(result)
                                )
                        else:
                            self.workflow_results[category] = result
                    else:
                        self.workflow_results[category] = WorkflowResult(
                            category=category,
                            status="failed",
                            error="æœªè·å–åˆ°æ‰§è¡Œç»“æœ"
                        )
                        
            except asyncio.TimeoutError:
                print(f"â° å¹¶è¡Œæ‰§è¡Œè¶…æ—¶ï¼Œå–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡")
                # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                for category, task in tasks.items():
                    if not task.done():
                        task.cancel()
                        # å°è¯•ç­‰å¾…ä»»åŠ¡å–æ¶ˆå®Œæˆï¼Œé¿å…æ‚¬æŒ‚
                        try:
                            # ç»™ä¸€ç‚¹æ—¶é—´è®©ä»»åŠ¡å“åº”å–æ¶ˆ
                            # await asyncio.wait_for(task, timeout=2.0) 
                            # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½await taskï¼Œå› ä¸ºtaskå¯èƒ½å› ä¸ºæ— æ³•å“åº”å–æ¶ˆè€Œå¡ä½
                            pass
                        except:
                            pass
                            
                        self.workflow_results[category] = WorkflowResult(
                            category=category,
                            status="timeout",
                            error="æ‰§è¡Œè¶…æ—¶"
                        )
            except Exception as e:
                print(f"âŒ å¹¶è¡Œæ‰§è¡Œå‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
                for category in categories:
                    if category not in self.workflow_results:
                        self.workflow_results[category] = WorkflowResult(
                            category=category,
                            status="failed",
                            error=f"æ‰§è¡Œå™¨é”™è¯¯: {str(e)}"
                        )
            
            # ç­‰å¾…æµå¼å“åº”è§£æä»»åŠ¡å®Œæˆ
            if streaming_task:
                try:
                    # æ¿€è¿›çš„æ¸…ç†ç­–ç•¥ï¼šæ— è®ºçŠ¶æ€å¦‚ä½•ï¼Œå¼ºåˆ¶å–æ¶ˆ
                    if not streaming_task.done():
                        streaming_task.cancel()
                        # å°è¯•ç­‰å¾…å–æ¶ˆå®Œæˆï¼Œä½†è®¾ç½®æçŸ­è¶…æ—¶
                        try:
                            # å¢åŠ ä¸€ä¸ªæçŸ­çš„waitï¼Œè®©EventLoopæœ‰æœºä¼šå¤„ç†cancelä¿¡å·
                            await asyncio.wait_for(streaming_task, timeout=0.1)
                        except (asyncio.CancelledError, asyncio.TimeoutError):
                            pass
                        except Exception as e:
                            print(f"âš ï¸ æµå¼ä»»åŠ¡å–æ¶ˆæ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
                            
                    # ç¡®ä¿å®Œå…¨é‡Šæ”¾å¼•ç”¨
                    streaming_task = None
                except Exception as e:
                    print(f"âš ï¸ æµå¼å“åº”è§£ææ¸…ç†å¼‚å¸¸: {str(e)}")
                        
        # æ‰“å°æ‰§è¡Œæ‘˜è¦
        print(f"\nğŸ“Š å¹¶è¡Œæ‰§è¡Œå®Œæˆæ‘˜è¦:")
        for category, result in self.workflow_results.items():
            status_emoji = "âœ…" if result.status == "completed" else "âŒ" if result.status == "failed" else "â°"
            print(f"  {status_emoji} {category}: {result.status} ({result.execution_time:.2f}s)")
            
        return self.workflow_results
    
    def clear_results(self):
        """æ¸…ç†æ‰§è¡Œç»“æœå’Œä¸Šä¸‹æ–‡"""
        self.workflow_tasks.clear()
        self.workflow_results.clear()
        self.workflow_contexts.clear()
        # æ¸…ç†åˆ†ç±»ä¸Šä¸‹æ–‡
        self.context_manager.clear_all_contexts()
        print("ğŸ§¹ å·²æ¸…ç†æ‰€æœ‰å·¥ä½œæµç¨‹æ‰§è¡Œç»“æœå’Œä¸Šä¸‹æ–‡")


class ReActAgent(Workflow):
    def __init__(
            self,
            *args: Any,
            llm: LLM | None = None,
            tools: list[BaseTool] | None = None,
            extra_context: str | None = None,
            react_chat_system_header: str | None = CUSTOM_REACT_CHAT_SYSTEM_HEADER,
            context_react_chat_system_header: str | None = CUSTOM_CONTEXT_REACT_CHAT_SYSTEM_HEADER,
            conclusion_prompt: str | None = CONCLUSION_PROMPT_TEMPLATE,
            filter_prompt: str | None = FILTER_PROMPT_TEMPLATE,
            planning_judge_prompt: str | None = PLANNING_JUDGE_TEMPLATE,
            planning_prompt: str | None = PLANNING_TEMPLATE,
            plan_modify_prompt=PLAN_MODIFY_TEMPLATE,
            plan_update_prompt=PLAN_UPDATE_TEMPLATE,
            conclusion_llm: LLM | None = None,
            filter_llm: LLM | None = None,
            planning_llm: LLM | None = None,
            planning_jugde_llm: LLM | None = None,
            plan_modify_llm: LLM | None = None,
            plan_update_llm: LLM | None = None,
            entity_recognition_llm: LLM | None = None,
            intent_recognition_llm: LLM | None = None,
            workflow_strategy: WorkflowStrategy | None = None,
            workflow_category: str | None = None,  # æ–°å¢ï¼šå·¥ä½œæµç¨‹åˆ†ç±»å‚æ•°
            **kwargs: Any,
    ) -> None:
        super().__init__(*args, **kwargs)
        self.tools = tools or []
        self.llm = llm or customDashscopeLLM()  # ä½¿ç”¨è‡ªå®šä¹‰LLMæ›¿ä»£OpenAI
        self.mcp_client = MCPClient()  # æ·»åŠ MCPå®¢æˆ·ç«¯
        self.workflow_strategy = workflow_strategy or DefaultWorkflowStrategy()  # é»˜è®¤ç­–ç•¥
        self.step_counter = 0  # æ­¥éª¤è®¡æ•°å™¨
        
        # è®¾ç½®å·¥ä½œæµç¨‹åˆ†ç±»ï¼ˆå°†åœ¨æ„å›¾è¯†åˆ«ä¸­åŠ¨æ€è°ƒæ•´ï¼‰
        self.workflow_category = workflow_category
        
        # self.formatter = ReActChatFormatter.from_defaults(
        #     context=extra_context or ""
        # )
        self.formatter = customReActChatFormatter(
            react_chat_system_header=react_chat_system_header,
            context_react_chat_system_header=context_react_chat_system_header).from_custom(
            context=extra_context or "",
        )
        self.output_parser = customReActOutputParser()

        # define formatters
        self.filter_formatter = FilterChatFormatter()
        self.conclusion_formatter = ConclusionChatFormatter()
        self.planning_judge_formatter = PlanningJudgeFormatter()
        self.planning_formatter = PlanningFormatter()
        self.plan_modify_formatter = PlanModifyFormatter()
        self.plan_update_formatter = PlanUpdateFormatter()
        self.entity_recognition_formatter = EntityRecognitionFormatter()
        self.intent_recognition_formatter = IntentRecognitionFormatter()

        # define llms - ç¡®ä¿æ‰€æœ‰LLMå®ä¾‹éƒ½æœ‰é»˜è®¤å€¼ï¼Œé¿å…NoneTypeé”™è¯¯
        self.conclusion_llm = conclusion_llm or self.llm
        self.filter_llm = filter_llm or self.llm
        self.planning_llm = planning_llm or self.llm
        self.planning_judge_llm = planning_jugde_llm or self.llm
        self.plan_modify_llm = plan_modify_llm or self.llm
        self.plan_update_llm = plan_update_llm or self.llm
        self.entity_recognition_llm = entity_recognition_llm or self.llm  # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤LLM
        self.intent_recognition_llm = intent_recognition_llm or self.llm  # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤LLM
        
        # åˆ›å»ºæµå¼å“åº”è§£æå™¨
        self.response_parser = StreamingResponseParser()

        # çº¿ç¨‹å®‰å…¨ï¼šä¸ºfilter_llmè°ƒç”¨æ·»åŠ äº’æ–¥é”
        import threading
        self._filter_llm_lock = threading.Lock()
        
        # åˆå§‹åŒ–å¹¶è¡Œå·¥ä½œæµç¨‹ç®¡ç†å™¨
        self.parallel_manager = ParallelWorkflowManager(self)
    
    def _filter_single_chunk_sync(self, chunk: str, query: str, chunk_index: int, batch_id: int, local_filter_llm) -> tuple[str, bool, int]:
        """
        åŒæ­¥æ–¹å¼è¿‡æ»¤å•ä¸ªæ–‡æ¡£å—ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰
        
        Args:
            chunk: è¦è¿‡æ»¤çš„æ–‡æ¡£å—
            query: ç”¨æˆ·æŸ¥è¯¢
            chunk_index: å—ç´¢å¼•
            batch_id: æ‰¹æ¬¡ID
            
        Returns:
            (chunk, is_relevant) å…ƒç»„
        """
        import datetime
        import asyncio
        
        try:
            # æ ¼å¼åŒ–è¾“å…¥ - æ£€æŸ¥è¿”å›ç±»å‹å¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            llm_input = self.filter_formatter.format(query=query, doc_chunks=chunk)
            
            # å¦‚æœè¿”å›çš„æ˜¯ChatMessageåˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            if isinstance(llm_input, list):
                # å°†ChatMessageåˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                formatted_text = ""
                for msg in llm_input:
                    if hasattr(msg, 'content'):
                        formatted_text += f"{msg.role}: {msg.content}\n"
                    else:
                        formatted_text += str(msg) + "\n"
                llm_input_text = formatted_text.strip()
            else:
                llm_input_text = str(llm_input)
                
            print(f"  ğŸ“¤ çº¿ç¨‹{batch_id}-å—{chunk_index+1} å¼€å§‹LLMè°ƒç”¨ [{datetime.datetime.now().strftime('%H:%M:%S.%f')[:-3]}]")
                
            # ä½¿ç”¨åŒæ­¥æ–¹å¼è°ƒç”¨LLM - ç›´æ¥ä¼ é€’å­—ç¬¦ä¸²ï¼ˆä½¿ç”¨çº¿ç¨‹ä¸“å±å®ä¾‹ï¼‰
            response = local_filter_llm.complete(llm_input_text)
            response_content = response.text
            final_content = self.response_parser.extract_answer(response_content)
            thinking_process = self.response_parser.extract_thinking(response_content)


            # åˆ¤æ–­ç›¸å…³æ€§
            is_relevant = "ç›¸å…³" in final_content and "æ— å…³" not in final_content
            import re
            score_match = re.search(r"SCORE\s*:\s*(\d{1,3})", final_content)
            score_json_match = re.search(r"\{\s*\"score\"\s*:\s*(\d{1,3})\s*\}", final_content)
            score_val = None
            if score_match:
                try:
                    score_val = int(score_match.group(1))
                except Exception:
                    score_val = None
            elif score_json_match:
                try:
                    score_val = int(score_json_match.group(1))
                except Exception:
                    score_val = None
            if score_val is None:
                score_val = 80 if is_relevant else 20
            if score_val < 0:
                score_val = 0
            if score_val > 100:
                score_val = 100
                
            if is_relevant:
                print(f"âœ…çº¿ç¨‹ {batch_id} - å—{chunk_index+1}: ç›¸å…³ ï¼Œscoreï¼š{score_val} [{datetime.datetime.now().strftime('%H:%M:%S.%f')[:-3]}]")
                print(f"  ğŸ§  æ€è€ƒè¿‡ç¨‹: {thinking_process}")
                print(f"  ğŸ“š çŸ¥è¯†ç‰‡æ®µ: {chunk}")
            else:
                print(f"âŒçº¿ç¨‹ {batch_id} - å—{chunk_index+1}: æ— å…³ ï¼Œscoreï¼š{score_val} [{datetime.datetime.now().strftime('%H:%M:%S.%f')[:-3]}]")
                print(f"  ğŸ§  æ€è€ƒè¿‡ç¨‹: {thinking_process}")
                print(f"  ğŸ“š çŸ¥è¯†ç‰‡æ®µ: {chunk}")
                
            try:
                if hasattr(self, "_event_loop") and hasattr(self, "workflow_strategy"):
                    asyncio.run_coroutine_threadsafe(
                        self.workflow_strategy.on_filter_progress(
                            batch_id,
                            chunk_index,
                            chunk,
                            is_relevant,
                            thinking_process,
                            getattr(self, "_current_category", ""),
                            score_val
                        ),
                        self._event_loop
                    )
            except Exception:
                pass
            return chunk, is_relevant, score_val
                
        except Exception as e:
            print(f"âš ï¸ çº¿ç¨‹ {batch_id} - å—{chunk_index+1} å¤„ç†å¤±è´¥: {e}")
            return chunk, True, 50
    
    def _filter_batch_in_thread_sync(self, chunks: List[str], query: str, batch_id: int) -> List[tuple[str, int]]:
        """
        åœ¨å•ç‹¬çº¿ç¨‹ä¸­åŒæ­¥å¤„ç†ä¸€æ‰¹æ–‡æ¡£å—çš„ç›¸å…³æ€§è¿‡æ»¤
        è¿™ä¸ªæ–¹æ³•ä¼šåœ¨ç‹¬ç«‹çš„çº¿ç¨‹ä¸­è¿è¡Œï¼ŒçœŸæ­£å®ç°æ‰¹æ¬¡é—´çš„å¹¶è¡Œ
        
        Args:
            chunks: è¦è¿‡æ»¤çš„æ–‡æ¡£å—åˆ—è¡¨
            query: ç”¨æˆ·æŸ¥è¯¢
            batch_id: æ‰¹æ¬¡IDï¼Œç”¨äºæ—¥å¿—
            
        Returns:
            ç›¸å…³çš„æ–‡æ¡£å—åˆ—è¡¨
        """
        # æ£€æŸ¥filter_llmæ˜¯å¦å¯ç”¨
        if not self.filter_llm:
            print(f"çº¿ç¨‹ {batch_id}: filter_llm æœªé…ç½®ï¼Œè¿”å›æ‰€æœ‰å—")
            return chunks
        
        import datetime, threading
        from custom_dashscope_llm import customDashscopeLLM
        from config import config

        # ä¸ºè¯¥çº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„LLMå®ä¾‹ï¼Œé¿å…å…±äº«å®¢æˆ·ç«¯å¸¦æ¥çš„çº¿ç¨‹å®‰å…¨é—®é¢˜
        local_filter_llm = customDashscopeLLM(
            model_code=getattr(config, 'FILTER_MODEL_NAME', config.DEFAULT_MODEL_NAME),
            api_key=config.DASHSCOPE_API_KEY,
            temperature=getattr(config, 'DEFAULT_TEMPERATURE', 0.01),
            top_p=getattr(config, 'DEFAULT_TOP_P', 0.01),
            context_window=getattr(config, 'DEFAULT_CONTEXT_WINDOW', 16384),
            max_tokens=getattr(config, 'DEFAULT_NUM_OUTPUT', 4096)
        )

        thread_name = threading.current_thread().name
        print(f"ğŸ” çº¿ç¨‹ {batch_id} ({thread_name}) å¼€å§‹å¤„ç† {len(chunks)} ä¸ªæ–‡æ¡£å—... [{datetime.datetime.now().strftime('%H:%M:%S.%f')[:-3]}]")
        
        relevant_chunks_with_scores = []
        
        # åœ¨è¿™ä¸ªçº¿ç¨‹ä¸­é¡ºåºå¤„ç†æ¯ä¸ªchunk
        for i, chunk in enumerate(chunks):
            try:
                chunk_result, is_relevant, score_val = self._filter_single_chunk_sync(chunk, query, i, batch_id, local_filter_llm)
                if is_relevant:
                    relevant_chunks_with_scores.append((chunk_result, score_val))
                    print(f"  âœ“ çº¿ç¨‹ {batch_id} - å—{i+1}: ç›¸å…³")
                else:
                    print(f"  âœ— çº¿ç¨‹ {batch_id} - å—{i+1}: æ— å…³")
            except Exception as e:
                print(f"âš ï¸ çº¿ç¨‹ {batch_id} - å—{i+1} å¤„ç†å¤±è´¥: {e}")
                relevant_chunks_with_scores.append((chunk, 50))
        
        print(f"âœ… çº¿ç¨‹ {batch_id} ({thread_name}) å®Œæˆï¼Œç­›é€‰å‡º {len(relevant_chunks_with_scores)}/{len(chunks)} ä¸ªç›¸å…³å—")
        relevant_chunks_with_scores.sort(key=lambda x: x[1], reverse=True)
        return relevant_chunks_with_scores
    
    async def _filter_chunk_batch_with_threads(self, chunks: List[str], query: str, batch_id: int) -> List[str]:
        """
        å¼‚æ­¥åŒ…è£…å™¨ï¼Œç”¨äºåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ‰¹æ¬¡å¤„ç†
        """
        loop = asyncio.get_event_loop()
        self._event_loop = loop
        self._current_category = ""
        return await loop.run_in_executor(
            None,  # ä½¿ç”¨é»˜è®¤çº¿ç¨‹æ± 
            self._filter_batch_in_thread_sync,
            chunks, query, batch_id
        )
    
    async def _filter_chunks_parallel(self, doc_chunks: List[str], query: str, category: str = "research-general") -> List[str]:
        """
        çœŸæ­£å¹¶è¡Œè¿‡æ»¤æ–‡æ¡£å—çš„ç›¸å…³æ€§
        æ¯ä¸ªæ‰¹æ¬¡åœ¨ç‹¬ç«‹çš„çº¿ç¨‹ä¸­è¿è¡Œï¼Œå®ç°çœŸæ­£çš„çº¿ç¨‹çº§å¹¶è¡Œ
        
        Args:
            doc_chunks: æ‰€æœ‰æ–‡æ¡£å—
            query: ç”¨æˆ·æŸ¥è¯¢
            category: å·¥ä½œæµåˆ†ç±»
            
        Returns:
            è¿‡æ»¤åçš„ç›¸å…³æ–‡æ¡£å—åˆ—è¡¨
        """
        if not doc_chunks:
            return []
        
        import datetime
        start_time = datetime.datetime.now()
        
        # å‘é€è¿‡æ»¤å¼€å§‹äº‹ä»¶
        await self.workflow_strategy.on_filter_start(len(doc_chunks), query, category)
        
        # æ¯ä¸ªçº¿ç¨‹å¤„ç†çš„chunkæ•°é‡
        chunks_per_thread = 3
        # æœ€å¤šä½¿ç”¨çš„çº¿ç¨‹æ•°
        max_threads = 3
        
        # è®¡ç®—å®é™…éœ€è¦çš„çº¿ç¨‹æ•°
        total_chunks = len(doc_chunks)
        needed_threads = min(max_threads, (total_chunks + chunks_per_thread - 1) // chunks_per_thread)
        
        # å°†æ–‡æ¡£å—å‡åŒ€åˆ†é…ç»™çº¿ç¨‹
        batches = []
        if needed_threads == 1:
            # å¦‚æœåªéœ€è¦ä¸€ä¸ªçº¿ç¨‹ï¼Œç›´æ¥å¤„ç†æ‰€æœ‰å—
            batches.append(doc_chunks)
        else:
            # è®¡ç®—æ¯ä¸ªçº¿ç¨‹çš„å®é™…è´Ÿè½½
            base_size = total_chunks // needed_threads
            remainder = total_chunks % needed_threads
            
            start_idx = 0
            for i in range(needed_threads):
                # å‰é¢çš„çº¿ç¨‹å¤šå¤„ç†ä¸€ä¸ªchunkï¼ˆå¦‚æœæœ‰ä½™æ•°ï¼‰
                current_size = base_size + (1 if i < remainder else 0)
                end_idx = start_idx + current_size
                if start_idx < total_chunks:
                    batches.append(doc_chunks[start_idx:end_idx])
                start_idx = end_idx
        
        print(f"ğŸ“Š æ–‡æ¡£å—æ€»æ•°: {total_chunks}, ä½¿ç”¨ {len(batches)} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†")
        for i, batch in enumerate(batches):
            print(f"  çº¿ç¨‹ {i+1}: {len(batch)} ä¸ªæ–‡æ¡£å—")
        
        # ä½¿ç”¨asyncioçš„çº¿ç¨‹æ± æ‰§è¡Œå™¨å®ç°çœŸæ­£çš„çº¿ç¨‹å¹¶è¡Œ
        loop = asyncio.get_event_loop()
        
        # åˆ›å»ºçº¿ç¨‹ä»»åŠ¡
        tasks = []
        for i, batch in enumerate(batches):
            if batch:  # ç¡®ä¿æ‰¹æ¬¡ä¸ä¸ºç©º
                # æ¯ä¸ªæ‰¹æ¬¡åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ
                task = loop.run_in_executor(
                    None,  # ä½¿ç”¨é»˜è®¤çº¿ç¨‹æ± 
                    self._filter_batch_in_thread_sync,
                    batch, query, i + 1
                )
                tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        if tasks:
            print(f"ğŸš€ å¯åŠ¨ {len(tasks)} ä¸ªå¹¶è¡Œçº¿ç¨‹...")
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # åˆå¹¶ç»“æœ
            all_relevant_chunks_with_scores = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"âŒ çº¿ç¨‹ {i+1} æ‰§è¡Œå¤±è´¥: {result}")
                    if i < len(batches):
                        for c in batches[i]:
                            all_relevant_chunks_with_scores.append((c, 50))
                else:
                    all_relevant_chunks_with_scores.extend(result)
            
            end_time = datetime.datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            filtered_count = total_chunks - len(all_relevant_chunks_with_scores)
            print(f"âœ… æ‰€æœ‰çº¿ç¨‹å®Œæˆï¼æ€»è€—æ—¶: {duration:.2f}ç§’, ç­›é€‰å‡º {len(all_relevant_chunks_with_scores)}/{total_chunks} ä¸ªç›¸å…³å—")
            
            # å‘é€è¿‡æ»¤å®Œæˆäº‹ä»¶
            await self.workflow_strategy.on_filter_complete(
                total_chunks, 
                len(all_relevant_chunks_with_scores), 
                filtered_count, 
                category
            )
            all_relevant_chunks_with_scores.sort(key=lambda x: x[1], reverse=True)
            return [c for c, s in all_relevant_chunks_with_scores]
        else:
            # æ²¡æœ‰ä»»åŠ¡ï¼Œè¿”å›ç©ºåˆ—è¡¨
            await self.workflow_strategy.on_filter_complete(0, 0, 0, category)
            return []
    
    async def get_mcp_tool_descriptions(self, category: str = None) -> str:
        """è·å–MCPå·¥å…·æè¿°ï¼Œæ”¯æŒåŸºäºcategoryçš„å·¥å…·è¿‡æ»¤"""
        try:
            # ç¡®ä¿MCPå®¢æˆ·ç«¯å·²è¿æ¥
            await self.mcp_client.ensure_connected()
            
            # è·å–å·¥å…·åˆ—è¡¨
            response = await self.mcp_client.list_tools()
            
            # è·å–å½“å‰categoryå¯¹åº”çš„ç™½åå•å·¥å…·åˆ—è¡¨ï¼ˆä»…å±•ç¤ºå…è®¸çš„å·¥å…·ï¼‰
            allowed_tools = self._get_allowed_tools_for_category(category)

            # æ ¼å¼åŒ–å·¥å…·æè¿°ï¼Œä»…åŒ…å«ç™½åå•ä¸­çš„å·¥å…·ï¼›è‹¥æœªé…ç½®ç™½åå•åˆ™å±•ç¤ºå…¨éƒ¨
            tool_descriptions = []
            for tool in response:
                tool_name = tool['name']
                
                # ç™½åå•ç”Ÿæ•ˆï¼šä»…å½“åœ¨ç™½åå•ä¸­æ—¶å±•ç¤ºï¼›æœªé…ç½®ç™½åå•ï¼ˆNoneï¼‰åˆ™å±•ç¤ºå…¨éƒ¨
                if (isinstance(allowed_tools, list) and tool_name in allowed_tools) or (allowed_tools is None):
                    desc = f"Tool Name: {tool_name}\n"
                    desc += f"Description: {tool['description']}\n"
                    if 'inputSchema' in tool and tool['inputSchema']:
                        desc += f"Parameters: {tool['inputSchema']}\n"
                    tool_descriptions.append(desc)

            return "\n".join(tool_descriptions)
        except Exception as e:
            print(f"Error getting MCP tool descriptions: {e}")
            return "No tools available"
    
    def _get_allowed_tools_for_category(self, category: str = None) -> list | None:
        """æ ¹æ®categoryè·å–å…è®¸ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨ï¼ˆç™½åå•æ¨¡å¼ï¼‰ã€‚æœªé…ç½®åˆ™è¿”å›Noneä»£è¡¨å…è®¸å…¨éƒ¨ã€‚"""
        from config import config
        
        mapping = getattr(config, 'TOOL_WHITELIST_MAPPING', {})
        if not category or category not in mapping:
            return mapping.get("default", None)
        return mapping[category]

    async def format_with_mcp_tools(self, chat_history, current_reasoning, current_plan, tool_descriptions, ctx=None, category=None):
        """ä½¿ç”¨MCPå·¥å…·æè¿°æ ¼å¼åŒ–èŠå¤©å†å²ï¼Œæ”¯æŒåŸºäºcategoryçš„å·¥å…·è¿‡æ»¤"""
        from llama_index.core.base.llms.types import ChatMessage, MessageRole
        
        # ä»ä¸Šä¸‹æ–‡è·å–å…ƒæ•°æ®ä¿¡æ¯
        metadata = {}
        if ctx:
            metadata = await ctx.store.get("input_metadata", default={})
        
        # æ„å»ºæ ¼å¼åŒ–å‚æ•°ï¼Œç›´æ¥ä½¿ç”¨tool_descriptions
        format_args = {
            "tool_desc": tool_descriptions,
            "tool_names": "MCPå·¥å…·",  # å ä½ç¬¦ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨MCP
            "current_plan": current_plan,
        }
        
        # å¦‚æœæœ‰å…ƒæ•°æ®ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
        if metadata:
            format_args["metadata_context"] = metadata
        else:
            format_args["metadata_context"] = "æ— é¢å¤–ä¿¡æ¯"
        
        if self.formatter.context:
            format_args["context"] = self.formatter.context

        fmt_sys_header = self.formatter.system_header.format(**format_args)

        # æ ¼å¼åŒ–æ¨ç†å†å²
        reasoning_history = []
        for reasoning_step in current_reasoning:
            if isinstance(reasoning_step, ObservationReasoningStep):
                message = ChatMessage(
                    role=MessageRole.TOOL,
                    content=reasoning_step.get_content(),
                )
            else:
                message = ChatMessage(
                    role=MessageRole.ASSISTANT,
                    content=reasoning_step.get_content(),
                )
            reasoning_history.append(message)

        return [
            ChatMessage(role=MessageRole.SYSTEM, content=fmt_sys_header),
            *chat_history,
            *reasoning_history,
        ]

    
    async def _merge_entities_to_metadata(self, ctx: Context, recognized_entities: list[dict]) -> None:
        """
        å°†è¯†åˆ«å‡ºçš„å®ä½“ä¿¡æ¯åˆå¹¶åˆ°å…ƒæ•°æ®ä¸­
        å¦‚æœå®ä½“è¯†åˆ«çš„ç»“æœåŒ…å«æ›´å‡†ç¡®çš„ä¿¡æ¯ï¼Œä¼šè¦†ç›–åŸæœ‰çš„ç©ºå€¼æˆ–nullå€¼
        """
        if not recognized_entities:
            return
            
        # è·å–ç°æœ‰çš„å…ƒæ•°æ®
        current_metadata = await ctx.store.get("input_metadata", default={})
        
        # åˆ›å»ºå®ä½“ä¿¡æ¯åˆ—è¡¨
        entity_info_list = []
        enhanced_metadata = current_metadata.copy()
        
        for entity in recognized_entities:
            device_name = entity.get("device_name", "")
            device_type = entity.get("device_type", "")
            fault_type = entity.get("fault_type", "")
            voltage_level = entity.get("voltage_level", "")
            
            # æ„å»ºå®ä½“ä¿¡æ¯å­—ç¬¦ä¸²
            entity_info = []
            if device_name:
                entity_info.append(f"è®¾å¤‡åç§°: {device_name}")
            if device_type:
                entity_info.append(f"è®¾å¤‡ç±»å‹: {device_type}")
            if fault_type:
                entity_info.append(f"æ•…éšœç±»å‹: {fault_type}")
            if voltage_level:
                entity_info.append(f"ç”µå‹ç­‰çº§: {voltage_level}")
                
            if entity_info:
                entity_info_list.append(", ".join(entity_info))
            
            # å¦‚æœè¯†åˆ«å‡ºçš„è®¾å¤‡åç§°ä¸ä¸ºç©ºï¼Œä¸”åŸå…ƒæ•°æ®ä¸­çš„dev_nameä¸ºç©ºæˆ–nullï¼Œåˆ™è¦†ç›–
            if device_name and (not enhanced_metadata.get("dev_name") or enhanced_metadata.get("dev_name") == "null"):
                enhanced_metadata["dev_name"] = device_name
                
            # å¦‚æœè¯†åˆ«å‡ºäº†æ•…éšœç±»å‹ï¼Œä¸”åŸå…ƒæ•°æ®ä¸­æ²¡æœ‰æˆ–ä¸ºç©ºï¼Œåˆ™è¦†ç›–
            if fault_type and not enhanced_metadata.get("fault_type1"):
                enhanced_metadata["fault_type1"] = fault_type
                

        
        # å°†è¯†åˆ«å‡ºçš„å®ä½“ä¿¡æ¯æ·»åŠ åˆ°å…ƒæ•°æ®ä¸­
        if entity_info_list:
            enhanced_metadata["recognized_entities"] = entity_info_list
            
        # æ›´æ–°ä¸Šä¸‹æ–‡ä¸­çš„å…ƒæ•°æ®
        await ctx.store.set("input_metadata", enhanced_metadata)
        
        if enhanced_metadata.get("dev_name") != current_metadata.get("dev_name"):
            print(f"  è®¾å¤‡åç§°å·²æ›´æ–°: {current_metadata.get('dev_name', 'null')} -> {enhanced_metadata.get('dev_name')}")

    @step
    async def new_user_msg(self, ctx: Context, ev: StartEvent) -> IntentRecognitionEvent:
        await self.workflow_strategy.on_step_start("new_user_msg", {
            "input": ev.input
        })
        
        # JSONè¾“å…¥è§£æ
        parsed_data = self._parse_user_input(ev.input)
        user_input = parsed_data["input"]
        metadata = parsed_data.get("metadata", {})
        attachments = parsed_data.get("attachments", [])
        
        # å°†è§£æåçš„å…ƒæ•°æ®å­˜å‚¨åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼Œä¾›åç»­æ¨ç†ä½¿ç”¨
        await ctx.store.set("input_metadata", metadata)
        await ctx.store.set("input_attachments", attachments)
        await ctx.store.set("user_input", user_input)  # å­˜å‚¨ç”¨æˆ·è¾“å…¥åˆ°ä¸Šä¸‹æ–‡

        
        # æ­£å¸¸çš„å·¥ä½œæµå¤„ç†é€»è¾‘
        # clear sources
        await ctx.store.set("sources", [])
        await ctx.store.set("plan_example", "")

        # init memory if needed
        memory = await ctx.store.get("memory", default=None)
        if not memory:
            memory = ChatMemoryBuffer.from_defaults(llm=self.llm)

        # get user input
        user_msg = ChatMessage(role="user", content=user_input)
        memory.put(user_msg)

        # clear current reasoning and current plan
        await ctx.store.set("current_reasoning", [])
        await ctx.store.set("current_plan", "")

        # clear some indicators
        await ctx.store.set("has_retrieved_plan_example", False)

        # set memory
        await ctx.store.set("memory", memory)
        
        result = IntentRecognitionEvent(input=user_input)
        
        await self.workflow_strategy.on_step_complete("new_user_msg", {
            "user_input": user_input,
            "metadata": metadata
        })
        
        return result
    
    def _parse_user_input(self, user_input: str) -> dict:
        """
        è§£æç”¨æˆ·è¾“å…¥ï¼Œæ”¯æŒJSONæ ¼å¼å’Œæ™®é€šæ–‡æœ¬æ ¼å¼
        """
        # å°è¯•è§£æä¸ºJSON
        try:
            # å¦‚æœè¾“å…¥çœ‹èµ·æ¥åƒJSONå­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
            if user_input.strip().startswith('{') and user_input.strip().endswith('}'):
                input_data = json.loads(user_input)
                return self._process_json_data(input_data)
        except json.JSONDecodeError:
            pass
        
        # å¦‚æœä¸æ˜¯JSONæˆ–è§£æå¤±è´¥ï¼ŒæŒ‰æ™®é€šæ–‡æœ¬å¤„ç†
        return {
            "input": user_input,
            "metadata": {},
            "attachments": []
        }
    
    def _process_json_data(self, input_data: dict) -> dict:
        """
        å¤„ç†JSONæ•°æ®ï¼Œè½¬æ¢æ—¶é—´æˆ³å¹¶é‡æ–°æ ¼å¼åŒ–
        æ”¯æŒä¸‰ç§æ ¼å¼ï¼š
        1. é€šç”¨æ ¼å¼: {"input": "...", "metadata": {...}, "attachments": [...]}
        2. å…¼å®¹æ ¼å¼: {"query": "...", "metadata": {...}, "attachments": [...]}
        3. é—ç•™æ ¼å¼å…¼å®¹: è‡ªåŠ¨æ˜ å°„æ—§å­—æ®µåˆ°é€šç”¨metadataç»“æ„
        """
        import time
        from datetime import datetime
        
        def format_date(input_str):
            """æ ¼å¼åŒ–æ—¥æœŸå­—ç¬¦ä¸²"""
            if not input_str:
                return ""
            
            # åˆ¤æ–­æ˜¯å¦ç¬¦åˆyyyy-MM-dd HH:mm:ssæ ¼å¼
            try:
                datetime.strptime(str(input_str), "%Y-%m-%d %H:%M:%S")
                return str(input_str)  # å¦‚æœç¬¦åˆæ—¥æœŸæ ¼å¼ï¼Œç›´æ¥è¿”å›
            except ValueError:
                pass
            
            # å°è¯•è§£æä¸ºæ—¶é—´æˆ³
            try:
                timestamp = int(input_str)
                # åˆ¤æ–­æ—¶é—´æˆ³é•¿åº¦ï¼ˆç§’çº§è¿˜æ˜¯æ¯«ç§’çº§ï¼‰
                if len(str(timestamp)) == 13:  # æ¯«ç§’çº§æ—¶é—´æˆ³
                    timestamp = timestamp / 1000
                elif len(str(timestamp)) == 10:  # ç§’çº§æ—¶é—´æˆ³
                    pass  # ä¿æŒåŸæ ·
                return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(timestamp))
            except (ValueError, OSError):
                return str(input_str)  # å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›åŸå­—ç¬¦ä¸²

        # é€šç”¨æ ¼å¼ä¼˜å…ˆ
        if isinstance(input_data, dict):
            if "input" in input_data or "metadata" in input_data or "attachments" in input_data:
                metadata = input_data.get("metadata", {})
                attachments = input_data.get("attachments", [])
                return {
                    "input": str(input_data.get("input") or ""),
                    "metadata": metadata if isinstance(metadata, dict) else {},
                    "attachments": attachments if isinstance(attachments, list) else [],
                }
            if "query" in input_data and ("metadata" in input_data or "attachments" in input_data):
                metadata = input_data.get("metadata", {})
                attachments = input_data.get("attachments", [])
                return {
                    "input": str(input_data.get("query") or ""),
                    "metadata": metadata if isinstance(metadata, dict) else {},
                    "attachments": attachments if isinstance(attachments, list) else [],
                }

        # æ—§æ ¼å¼å…¼å®¹ï¼šä¿ç•™æ—§å­—æ®µåæ˜ å°„ï¼Œä½†ä¸å†ç»‘å®šä¸ºâ€œæ•…éšœâ€è¯­ä¹‰
        legacy_metadata = {
            "event_time": format_date(input_data.get("occurTime", "")),
            "event_id": input_data.get("faultId", ""),
            "source_device_id": input_data.get("devId", ""),
            "source_device_name": input_data.get("devName", ""),
        }
        # æ¸…ç†ç©ºå€¼
        legacy_metadata = {k: v for k, v in legacy_metadata.items() if v}

        return {
            "input": str(input_data.get("faultDescr", "") or ""),
            "metadata": legacy_metadata,
            "attachments": input_data.get("attachments", []) if isinstance(input_data.get("attachments", []), list) else [],
        }
    
    @step
    async def intent_recognition(self, ctx: Context, ev: IntentRecognitionEvent) -> EntityRecognitionEvent | StopEvent:
        """
        æ„å›¾è¯†åˆ«æ­¥éª¤ï¼šä½¿ç”¨å¤§æ¨¡å‹åˆ¤æ–­æ˜¯å¦éœ€è¦å¿«é€Ÿå“åº”
        """
        await self.workflow_strategy.on_step_start("intent_recognition", {
            "input": ev.input
        })
        
        user_input = ev.input
        
        try:
            # ä½¿ç”¨æ„å›¾è¯†åˆ«æ¨¡æ¿è°ƒç”¨LLM
            llm_input = self.intent_recognition_formatter.format(user_input)
            
            print(f"âš¡ï¸è¯·æ±‚æ„å›¾è¯†åˆ«æ¨¡å‹å“åº”...")
            response_gen = await self.intent_recognition_llm.astream_chat(messages=llm_input)
            
            # å®šä¹‰å¼‚æ­¥å›è°ƒå‡½æ•°
            async def on_intent_thinking(content, metadata):
                # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
                current_category = await ctx.store.get("current_workflow_category", default=None)
                thinking_metadata = {"step": "æ„å›¾è¯†åˆ«æ€è€ƒ"}
                if current_category:
                    thinking_metadata["category"] = current_category
                await self.workflow_strategy.on_streaming_content(
                    content, "intent", "thinking", thinking_metadata
                )
            
            async def on_intent_content(content, metadata):
                # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
                current_category = await ctx.store.get("current_workflow_category", default=None)
                content_metadata = {"step": "æ„å›¾è¯†åˆ«è¾“å‡º"}
                if current_category:
                    content_metadata["category"] = current_category
                await self.workflow_strategy.on_streaming_content(
                    content, "intent", "output", content_metadata
                )
            
            # ä½¿ç”¨StreamingResponseParserè§£ææµå¼å“åº”
            response_content = await self.response_parser.parse_streaming_response(
                response_gen,
                on_thinking=on_intent_thinking,
                on_content=on_intent_content,
                thinking_metadata={"step": "æ„å›¾è¯†åˆ«æ€è€ƒ", "phase": "intent_recognition"},
                content_metadata={"step": "æ„å›¾è¯†åˆ«è¾“å‡º", "phase": "intent_recognition"}
            )


            final_content = self.response_parser.extract_final_content(response_content)
            
            # è§£æJSONå“åº”
            import re
            json_match = re.search(r'```json(.*?)```', final_content, re.DOTALL)
            if json_match:
                json_content = json_match.group(1)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONä»£ç å—ï¼Œå°è¯•ç›´æ¥è§£æ
                json_content = final_content.strip()
            
            # è§£æJSON
            try:
                intent_result = json.loads(json_content)
                is_quick_response = intent_result.get("is_quick_response", False)
                standard_answer = intent_result.get("standard_answer", "")
                workflow_categories = intent_result.get("workflow_categories", ["research-general"])
                
                # å°†å·¥ä½œæµç¨‹åˆ†ç±»ä¿¡æ¯å­˜å‚¨åˆ°ä¸Šä¸‹æ–‡ä¸­
                await ctx.store.set("workflow_categories", workflow_categories)
                
                print(f"ğŸ” æ„å›¾è¯†åˆ«ç»“æœ: å·¥ä½œæµç¨‹åˆ†ç±»={workflow_categories}")
                
            except json.JSONDecodeError as e:
                print(f"âš ï¸ æ„å›¾è¯†åˆ«JSONè§£æå¤±è´¥: {e}")
                print(f"å“åº”å†…å®¹: {response_content}")
                is_quick_response = False
                standard_answer = ""
                workflow_categories = []  # é»˜è®¤ä¸ºç©ºåˆ—è¡¨ï¼Œä¸è®¾ç½®é»˜è®¤åˆ†ç±»
                await ctx.store.set("workflow_categories", workflow_categories)
            
            # å¦‚æœéœ€è¦å¿«é€Ÿå“åº”ï¼Œç›´æ¥è¿”å›ç»“æœ
            if is_quick_response and standard_answer:
                # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
                current_category = await ctx.store.get("current_workflow_category", default=None)
                event_metadata = {
                    "standard_answer": standard_answer,
                    "user_input": user_input
                }
                if current_category:
                    event_metadata["category"] = current_category
                
                await self.workflow_strategy.on_workflow_event("quick_response_triggered", 
                    f"è§¦å‘å¿«é€Ÿå“åº” {standard_answer[:50]}...", event_metadata)
                
                await self.workflow_strategy.on_step_complete("intent_recognition", {
                    "quick_response_triggered": True,
                    "standard_answer": standard_answer
                })
                
                # ç›´æ¥è¿”å›StopEventï¼Œç»“æŸå·¥ä½œæµ
                return StopEvent(
                    result={
                        "response": standard_answer,
                        "sources": [],
                        "reasoning": [],
                        "quick_response": True
                    }
                )
            
            # å¦‚æœä¸éœ€è¦å¿«é€Ÿå“åº”ï¼Œç»§ç»­æ­£å¸¸æµç¨‹
            # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
            current_category = await ctx.store.get("current_workflow_category", default=None)
            event_metadata = {
                "quick_response_triggered": False,
                "user_input": user_input
            }
            if current_category:
                event_metadata["category"] = current_category
                
            await self.workflow_strategy.on_workflow_event("intent_recognition_complete", 
                "æ„å›¾è¯†åˆ«å®Œæˆï¼Œè¿›å…¥æ­£å¸¸æµç¨‹", event_metadata)
            
            await self.workflow_strategy.on_step_complete("intent_recognition", {
                "quick_response_triggered": False,
                "continue_normal_flow": True
            })
            
            return EntityRecognitionEvent(input=user_input)
            
        except Exception as e:
            print(f"âš ï¸ æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
            await self.workflow_strategy.on_step_complete("intent_recognition", {
                "error": str(e),
                "fallback_to_normal": True
            })
            
            return EntityRecognitionEvent(input=user_input)

    @step
    async def entity_recognition(self, ctx: Context, ev: EntityRecognitionEvent) -> EntityAnalysisEvent | StopEvent:
        """
        å®ä½“è¯†åˆ«æ­¥éª¤ï¼šè¯†åˆ«ç”¨æˆ·è¾“å…¥ä¸­çš„è®¾å¤‡å¹¶é€‰æ‹©åˆé€‚çš„å·¥ä½œæµæ¨¡æ¿
        """
        await self.workflow_strategy.on_step_start("entity_recognition", {
            "input": ev.input
        })
        
        # æ­£å¸¸çš„å®ä½“è¯†åˆ«å¤„ç†
        user_input = ev.input
        
        try:
            # ä½¿ç”¨å®ä½“è¯†åˆ«æ¨¡æ¿è°ƒç”¨LLM
            llm_input = self.entity_recognition_formatter.format(user_input)
            
            print(f"âš¡ï¸è¯·æ±‚å®ä½“è¯†åˆ«æ¨¡å‹å“åº”...")
            response_gen = await self.entity_recognition_llm.astream_chat(messages=llm_input)
            
            # å®šä¹‰å¼‚æ­¥å›è°ƒå‡½æ•°
            async def on_entity_thinking(content, metadata):
                # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
                current_category = await ctx.store.get("current_workflow_category", default=None)
                thinking_metadata = {"step": "å®ä½“è¯†åˆ«æ€è€ƒ"}
                if current_category:
                    thinking_metadata["category"] = current_category
                await self.workflow_strategy.on_streaming_content(
                    content, "entity", "thinking", thinking_metadata
                )
            
            async def on_entity_content(content, metadata):
                # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
                current_category = await ctx.store.get("current_workflow_category", default=None)
                content_metadata = {"step": "å®ä½“è¯†åˆ«è¾“å‡º"}
                if current_category:
                    content_metadata["category"] = current_category
                await self.workflow_strategy.on_streaming_content(
                    content, "entity", "output", content_metadata
                )
            
            # ä½¿ç”¨StreamingResponseParserè§£ææµå¼å“åº”
            response_content = await self.response_parser.parse_streaming_response(
                response_gen,
                on_thinking=on_entity_thinking,
                on_content=on_entity_content,
                thinking_metadata={"step": "å®ä½“è¯†åˆ«æ€è€ƒ", "phase": "entity_recognition"},
                content_metadata={"step": "å®ä½“è¯†åˆ«è¾“å‡º", "phase": "entity_recognition"}
            )
            
            # è§£æJSONå“åº”
            import re
            final_content = self.response_parser.extract_final_content(response_content)
            # æå–JSONä»£ç å—ä¸­çš„å†…å®¹
            # åŒ¹é…å¸¦æˆ–ä¸å¸¦```jsonæ ‡è®°çš„JSONå†…å®¹
            json_match = re.search(r'```json\n?(.*?)\n?```|^\s*(\[.*\])\s*$', final_content, re.DOTALL)
            if json_match:
                json_content = json_match.group(1)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONä»£ç å—ï¼Œå°è¯•ç›´æ¥è§£æå“åº”å†…å®¹
                json_content = final_content.strip()

            
            # è§£æJSON
            try:
                recognized_entities = json.loads(json_content)
                # ç¡®ä¿ç»“æœæ˜¯åˆ—è¡¨ç±»å‹
                if not isinstance(recognized_entities, list):
                    if isinstance(recognized_entities, dict):
                        recognized_entities = [recognized_entities]
                    else:
                        recognized_entities = []
            except json.JSONDecodeError as e:
                print(f"\nâš ï¸ JSONè§£æå¤±è´¥: {e}")
                print(f"é¢„å¤„ç†åçš„JSONå†…å®¹: {json_content}")
                print(f"åŸå§‹å“åº”å†…å®¹: {response_content}")
                recognized_entities = []

            workflow_categories = await ctx.store.get("workflow_categories", default=["research-general"])
            if not workflow_categories:
                # å³ä½¿æ²¡æœ‰è¯†åˆ«å‡ºåˆ†ç±»ï¼Œä¹Ÿä¿ç•™ä¸ºç©ºåˆ—è¡¨ï¼Œç”±åç»­é€»è¾‘å¤„ç†
                workflow_categories = []
            
            if workflow_categories:
                current_category = workflow_categories[0]
                await ctx.store.set("current_workflow_category", current_category)
                selected_workflow_template = self._select_workflow_template(recognized_entities, current_category)
            else:
                current_category = None
                await ctx.store.set("current_workflow_category", None)
                selected_workflow_template = "" # ä¸ä½¿ç”¨ä»»ä½•æ¨¡æ¿

            
            # å°†è¯†åˆ«å‡ºçš„å®ä½“ä¿¡æ¯åˆå¹¶åˆ°å…ƒæ•°æ®ä¸­
            await self._merge_entities_to_metadata(ctx, recognized_entities)
            
            # å‘é€å®ä½“è¯†åˆ«å®Œæˆäº‹ä»¶
            # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
            current_category = await ctx.store.get("current_workflow_category", default=None)
            event_metadata = {
                "entities_count": len(recognized_entities),
                "recognized_entities": recognized_entities,
                "selected_workflow_template": selected_workflow_template
            }
            if current_category:
                event_metadata["category"] = current_category
                
            await self.workflow_strategy.on_workflow_event("entity_recognition_complete", 
                f"å®ä½“è¯†åˆ«å®Œæˆï¼Œè¯†åˆ«å‡º {len(recognized_entities)} ä¸ªå®ä½“", event_metadata)
            
            await self.workflow_strategy.on_step_complete("entity_recognition", {
                "recognized_entities": recognized_entities,
                "selected_workflow_template": selected_workflow_template,
                "entities_count": len(recognized_entities)
            })
            
            return EntityAnalysisEvent(
                input=user_input,
                recognized_entities=recognized_entities,
                selected_workflow_template=selected_workflow_template
            )
            
        except Exception as e:
            print(f"\nâš ï¸ å®ä½“è¯†åˆ«å¤±è´¥: {e}")
            # å¦‚æœå®ä½“è¯†åˆ«å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
            default_template = get_workflow_template_by_device_type("å…¶ä»–è®¾å¤‡", self.workflow_category)
            
            await self.workflow_strategy.on_step_complete("entity_recognition", {
                "error": str(e),
                "fallback_to_default": True,
                "selected_workflow_template": default_template
            })
            
            return EntityAnalysisEvent(
                input=user_input,
                recognized_entities=[],
                selected_workflow_template=default_template
            )
    
    def _select_workflow_template(self, recognized_entities: list[dict], current_workflow_category: str = None) -> str:
        """
        æ ¹æ®è¯†åˆ«å‡ºçš„å®ä½“å’Œå½“å‰å·¥ä½œæµç¨‹åˆ†ç±»é€‰æ‹©åˆé€‚çš„å·¥ä½œæµæ¨¡æ¿
        """
        # ä½¿ç”¨ä¼ å…¥çš„å·¥ä½œæµç¨‹åˆ†ç±»ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤çš„
        workflow_category = current_workflow_category or self.workflow_category
        
        if not recognized_entities:
            # å¦‚æœæ²¡æœ‰è¯†åˆ«å‡ºå®ä½“ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
            return get_workflow_template_by_device_type("å…¶ä»–è®¾å¤‡", workflow_category)
        
        # ç»Ÿè®¡è®¾å¤‡ç±»å‹
        device_type_counts = {}
        for entity in recognized_entities:
            device_type = entity.get("device_type", "å…¶ä»–è®¾å¤‡")
            device_type_counts[device_type] = device_type_counts.get(device_type, 0) + 1
        
        # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„è®¾å¤‡ç±»å‹
        if device_type_counts:
            primary_device_type = max(device_type_counts, key=device_type_counts.get)
            return get_workflow_template_by_device_type(primary_device_type, workflow_category)



    @step
    async def check_valid_plan(
            self, ctx: Context, ev: EntityAnalysisEvent
    ) -> PlanningEvent | StopEvent:
        '''
        In this function, we use the user query to judge whether the current plan is valid.
        We also store the entity analysis results for later use in planning.
        We support parallel execution of multiple workflow categories.
        '''

        # å°†å®ä½“åˆ†æç»“æœå­˜å‚¨åˆ°ä¸Šä¸‹æ–‡ä¸­
        await ctx.store.set("recognized_entities", ev.recognized_entities)
        await ctx.store.set("selected_workflow_template", ev.selected_workflow_template)

        print(f"\nğŸ” è¯†åˆ«å‡ºçš„è®¾å¤‡: {len(ev.recognized_entities)} ä¸ª")
        for entity in ev.recognized_entities:
            print(f"  - {entity.get('device_name', '')}: {entity.get('device_type', '')}, {entity.get('fault_type', 'æ— ')}")
        print(f"\nğŸ“„ é€‰æ‹©çš„å·¥ä½œæµæ¨¡æ¿: {ev.selected_workflow_template}...")

        # è·å–å·¥ä½œæµç¨‹åˆ†ç±»
        workflow_categories = await ctx.store.get("workflow_categories", default=["research-general"])
        if not workflow_categories:
            # å¦‚æœæ²¡æœ‰åˆ†ç±»ï¼Œä¹Ÿä¸é»˜è®¤å›é€€åˆ°é»˜è®¤åˆ†ç±»ï¼Œä¿æŒä¸ºç©º
            workflow_categories = []
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå·¥ä½œæµç¨‹åˆ†ç±»
        if len(workflow_categories) > 1:
            print(f"\nğŸš€ æ£€æµ‹åˆ°å¤šä¸ªå·¥ä½œæµç¨‹åˆ†ç±»: {workflow_categories}")
            print("ğŸ”„ å¯åŠ¨å¹¶è¡Œæ‰§è¡Œæ¨¡å¼...")
            
            try:
                # ä½¿ç”¨å¹¶è¡Œç®¡ç†å™¨æ‰§è¡Œå¤šä¸ªå·¥ä½œæµç¨‹
                # æ³¨æ„ï¼šexecute_parallel_workflows å†…éƒ¨ä½¿ç”¨ asyncio.create_task åˆ›å»ºä»»åŠ¡
                # ä½†åœ¨æ­¤å¤„æˆ‘ä»¬éœ€è¦ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆæ‰èƒ½è¿”å›ç»“æœ
                # å› æ­¤è¿™é‡Œå®é™…ä¸Šæ˜¯"å¹¶å‘"æ‰§è¡Œï¼Œå½“å‰ä¸»æµç¨‹ä¼šç­‰å¾…å¹¶å‘ç»“æœ
                parallel_results = await self.parallel_manager.execute_parallel_workflows(
                    ctx, ev, workflow_categories, timeout=config.WORKFLOW_EXECUTION_TIMEOUT,
                    on_thinking=self._on_parallel_thinking,
                    on_content=self._on_parallel_content
                )
                
                # åˆå¹¶å¹¶è¡Œæ‰§è¡Œçš„ç»“æœ
                combined_reasoning = []
                combined_sources = []
                
                for category, result in parallel_results.items():
                    if result.status == "completed":
                        if result.reasoning:
                            combined_reasoning.extend([f"[{category}] {r}" for r in result.reasoning])
                        if result.sources:
                            combined_sources.extend([f"[{category}] {s}" for s in result.sources])
                    else:
                        # å¤„ç†å¤±è´¥æˆ–è¶…æ—¶çš„æƒ…å†µ
                        error_msg = f"[{category}] æ‰§è¡Œ{result.status}: {result.error or 'æœªçŸ¥é”™è¯¯'}"
                        combined_reasoning.append(error_msg)
                
                # è¿”å›åˆå¹¶åçš„ç»“æœ
                return StopEvent(
                    result={
                        "response": f"å¹¶è¡Œæ‰§è¡Œå®Œæˆï¼Œå¤„ç†äº† {len(workflow_categories)} ä¸ªå·¥ä½œæµç¨‹åˆ†ç±»",
                        "reasoning": combined_reasoning,
                        "sources": combined_sources,
                        "parallel_results": parallel_results,
                        "workflow_category": "ç»¼åˆ" # æ ‡è®°ä¸ºç»¼åˆç»“æœ
                    }
                )
            except Exception as e:
                print(f"âŒ å¹¶è¡Œæ‰§è¡Œå‡ºé”™: {str(e)}")
                # å‡ºé”™æ—¶è¿”å›éƒ¨åˆ†ç»“æœæˆ–é”™è¯¯ä¿¡æ¯
                return StopEvent(
                    result={
                        "response": f"å¹¶è¡Œæ‰§è¡Œéƒ¨åˆ†å¤±è´¥: {str(e)}",
                        "reasoning": [f"æ‰§è¡Œé”™è¯¯: {str(e)}"],
                        "sources": [],
                        "workflow_category": "ç»¼åˆ"
                    }
                )

        # å•ä¸ªå·¥ä½œæµç¨‹çš„æƒ…å†µï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
        # è®¾ç½®å½“å‰å·¥ä½œæµç¨‹åˆ†ç±»
        if workflow_categories:
            current_category = workflow_categories[0]
            await ctx.store.set("current_workflow_category", current_category)
        else:
            # å¦‚æœæ²¡æœ‰åˆ†ç±»ï¼Œåˆ™ä¸è®¾ç½®å½“å‰åˆ†ç±»ï¼Œä¹Ÿä¸å›é€€åˆ°é»˜è®¤åˆ†ç±»
            current_category = None
            await ctx.store.set("current_workflow_category", None)
            print("âš ï¸ æœªè¯†åˆ«åˆ°å·¥ä½œæµç¨‹åˆ†ç±»ï¼Œå°†ä¸ä½¿ç”¨ç‰¹å®šæµç¨‹æ¨¡æ¿")
        
        return PlanningEvent(input=ev.input, additional_input=[])


    @step
    async def generate_plan(
            self, ctx: Context, ev: PlanningEvent
    ) -> InputEvent:
        await self.workflow_strategy.on_step_start("generate_plan", {
            "input": ev.input,
            "additional_input": ev.additional_input
        })
        '''
        In this function, we are sure that currently we do not have a valid plan accepted by the user.
        So we need to generate a plan given the user query and the chat history.
        After gnerating the plan, we need to tell the user, and wait for their feedback.
        Return the user feedback to ConciergeEvent, where the feedback is judged. Also, store the current plan in the context for later use.
        '''

        # get the memory
        memory = await ctx.store.get("memory")
        chat_history = memory.get()
        current_reasoning = await ctx.store.get("current_reasoning", default=[])
        usr_query = memory.get("usr_msg", default=None)

        # Get the user feedback, could be empty which means either this is the first round, or the user didn't provide any feedback.
        # In both empty cases, we generate a new plan.
        user_feedback = ev.additional_input

        query = ev.input  # get the user query

        # TODO: generate the plan and ask the user for feedback.

        # To generate the plan, we need prepare the following components:
        # 1. query
        # 2. current plan, if any; together with the user feedback
        # 3. some plan stored in the database/history as example
        # 4. tool list, optional
        query = ev.input
        current_plan = await ctx.store.get("current_plan", default="")  # get the current plan from the context
        has_retrieved_plan_example = await ctx.store.get("has_retrieved_plan_example",
                                                   default=False)  # flag to avoid always no example is retrieved, but we still try to retrieve.

        # è·å–è®¡åˆ’ç¤ºä¾‹ï¼Œå¦‚æœæœ‰çš„è¯
        plan_example = await ctx.store.get("plan_example", default="")
        if not plan_example and not has_retrieved_plan_example:
            try:
                from tools import retrieve_plans_from_db
                current_category = await ctx.store.get("current_workflow_category", default=None)
                plan_example = await retrieve_plans_from_db(query, category=current_category)
            except Exception as e:
                print(f"Error retrieving plan examples: {e}")
                plan_example = []
            await ctx.store.set("has_retrieved_plan_example", True)

            # if we have similar plan examples. Note there is situiation where we have no similar examples given threshold.
            # we store the plan example in the context, no need for further retrieval
            if len(plan_example) > 0:
                await ctx.store.set("plan_example", plan_example)

        # generate plan
        generated_plan = ""
        # if current_plan and user_feedback:
            # if there is current_plan, we just modify it according to the user feedback
            # modification_input = self.plan_modify_formatter.format(
            #     current_plan=current_plan,
            #     modify_suggestion=user_feedback,
            # )

            # print(f"âŒ›ï¸è¯·æ±‚{config.PLANNING_MODEL_NAME}æ¨¡å‹å“åº”...")
            # response_gen = await self.plan_modify_llm.astream_chat(messages=modification_input)

            # async for response in response_gen:
            #     if hasattr(response, 'delta') and response.delta:
            #         print(response.delta, end='', flush=True)
            #
            # # Extract the plan from the complete response
            # generated_plan = response.message.content.split("\n" + "=" * 20 + "å®Œæ•´å›å¤" + "=" * 20 + "\n")[-1]

        if current_plan:
            # ç”¨æˆ·æ²¡æœ‰æä¾›åé¦ˆï¼Œç›´æ¥è¿”å›ç°æœ‰è®¡åˆ’å¹¶å¼€å§‹æ‰§è¡Œ
            # user does not provide feedback for current plan, just return it. Ask user to comment on current plan.
            # print("â¬‡ï¸æˆ‘ç°åœ¨çš„æ–¹æ¡ˆå¦‚ä¸‹ã€‚ä½ è¿˜æ²¡æœ‰å¯¹ç°åœ¨çš„æ–¹æ¡ˆä½œå‡ºåé¦ˆï¼Œå¦‚æœä½ éœ€è¦ä»»ä½•æ”¹åŠ¨ï¼Œè¯·åœ¨å¼€å§‹ç ”ç©¶å‰å‘Šè¯‰æˆ‘ï¼š\n\n",
            #       current_plan)
            # question = "æ˜¯å¦å¼€å§‹ç ”ç©¶ï¼Ÿ"
            # user_feedback = input(question + "\n\n>")
            # return ConciergeEvent(input=usr_query[0].content, additional_input=[user_feedback])
            
            print("âœ…ä½¿ç”¨ç°æœ‰è®¡åˆ’ï¼Œå¼€å§‹æ‰§è¡Œ...")
            return InputEvent()

        else:
            # there is no current plan
            assert (usr_query is not None)

            # get tool description from MCP
            current_workflow_category = await ctx.store.get("current_workflow_category", default=None)
            tool_desc = await self.get_mcp_tool_descriptions(current_workflow_category)
            metadata_context = await ctx.store.get("input_metadata", "")
            # metadata_context
            # ä½¿ç”¨ä»å®ä½“è¯†åˆ«ä¸­è·å¾—çš„åŠ¨æ€å·¥ä½œæµæ¨¡æ¿
            selected_workflow_template = await ctx.store.get("selected_workflow_template", default="")

            
            llm_input = self.planning_formatter.format(
                query=usr_query,
                plan_examples=plan_example,
                tool_desc=tool_desc,
                workflow=selected_workflow_template,  # ä½¿ç”¨åŠ¨æ€é€‰æ‹©çš„å·¥ä½œæµç¨‹æ¨¡æ¿
                metadata_context=metadata_context,
            )

            response_gen = await self.planning_llm.astream_chat(
                messages=llm_input
            )

            # å®šä¹‰å¼‚æ­¥å›è°ƒå‡½æ•°
            async def on_planning_thinking(content, metadata):
                # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
                current_category = await ctx.store.get("current_workflow_category", default=None)
                thinking_metadata = {"step": "è®¡åˆ’ç”Ÿæˆæ€è€ƒ"}
                if current_category:
                    thinking_metadata["category"] = current_category
                await self.workflow_strategy.on_streaming_content(
                    content, "planning", "thinking", thinking_metadata
                )
            
            async def on_planning_content(content, metadata):
                # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
                current_category = await ctx.store.get("current_workflow_category", default=None)
                content_metadata = {"step": "è®¡åˆ’ç”Ÿæˆè¾“å‡º"}
                if current_category:
                    content_metadata["category"] = current_category
                await self.workflow_strategy.on_streaming_content(
                    content, "planning", "output", content_metadata
                )
            
            # ä½¿ç”¨StreamingResponseParserè§£ææµå¼å“åº”
            response_content = await self.response_parser.parse_streaming_response(
                response_gen,
                on_thinking=on_planning_thinking,
                on_content=on_planning_content,
                thinking_metadata={"step": "è®¡åˆ’ç”Ÿæˆæ€è€ƒ", "phase": "planning"},
                content_metadata={"step": "è®¡åˆ’ç”Ÿæˆè¾“å‡º", "phase": "planning"}
            )

            # ä»å“åº”ä¸­æå–è®¡åˆ’
            generated_plan = self.response_parser.extract_final_content(response_content)

        # write current plan to context
        await ctx.store.set("current_plan", generated_plan)
        current_plan = generated_plan

        # å°†ç”Ÿæˆçš„è®¡åˆ’å†™å…¥æ•°æ®åº“ï¼Œä¾›åç»­æ£€ç´¢ä½¿ç”¨
        if generated_plan.strip():
            try:
                from tools import write_plans_to_db
                current_category = await ctx.store.get("current_workflow_category", default=None)
                await write_plans_to_db(query, generated_plan, category=current_category)
                print("âœ…è®¡åˆ’å·²å†™å…¥æ•°æ®åº“")
            except Exception as e:
                print(f"âš ï¸ å†™å…¥è®¡åˆ’åˆ°æ•°æ®åº“å¤±è´¥: {e}")

        # å‘é€è®¡åˆ’ç”Ÿæˆå®Œæˆäº‹ä»¶
        # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
        current_category = await ctx.store.get("current_workflow_category", default=None)
        event_metadata = {
            "plan_generated": bool(generated_plan),
            "plan_content": generated_plan[:200] + "..." if len(generated_plan) > 200 else generated_plan
        }
        if current_category:
            event_metadata["category"] = current_category
            
        await self.workflow_strategy.on_workflow_event("plan_generation_complete", 
            "è®¡åˆ’ç”Ÿæˆå®Œæˆ", event_metadata)
        
        await self.workflow_strategy.on_step_complete("generate_plan", {
            "plan_generated": bool(generated_plan)
        })

        # æ³¨é‡Šæ‰ç”¨æˆ·åé¦ˆé€»è¾‘ï¼Œç›´æ¥å¼€å§‹æ‰§è¡Œ
        # TODO: ask user for feedback
        # print("â¬‡ï¸è¿™æ˜¯æˆ‘æ‹Ÿå®šçš„æ–¹æ¡ˆã€‚å¦‚æœä½ éœ€è¦è¿›è¡Œä»»ä½•æ”¹åŠ¨ï¼Œè¯·åœ¨æˆ‘å¼€å§‹ç ”ç©¶å‰å‘Šè¯‰æˆ‘ã€‚\n\n", current_plan)
        # question = "æ˜¯å¦å¼€å§‹ç ”ç©¶ï¼Ÿ"
        # usr_response = await ctx.wait_for_event(
        #     HumanResponseEvent,
        #     waiter_id=question,
        #     waiter_event=InputRequiredEvent(
        #         prefix=question,
        #     ),
        # )

        # # collect the user feedback
        # user_feedback += [usr_response.response]

        # user_feedback = input(question + "\n\n>")
        
        # ç›´æ¥è¿”å›PrepEventï¼Œè·³è¿‡ç”¨æˆ·ç¡®è®¤
        print("âœ…è®¡åˆ’ç”Ÿæˆå®Œæˆï¼Œå¼€å§‹æ‰§è¡Œ...")
        return PrepEvent()

    # æ³¨é‡Šæ‰handle_user_feedbackæ–¹æ³•ï¼Œå› ä¸ºä¸å†éœ€è¦ç”¨æˆ·ç¡®è®¤
    # @step
    # async def handle_user_feedback(
    #         self, ctx: Context, ev: ConciergeEvent
    # ) -> PrepEvent | PlanningEvent:
    #     """
    #     å¤„ç†ç”¨æˆ·åé¦ˆï¼Œåˆ¤æ–­ç”¨æˆ·æ˜¯å¦æ¥å—è®¡åˆ’å¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
    #     """
    #     await self.workflow_strategy.on_step_start("handle_user_feedback", {
    #         "input": ev.input,
    #         "additional_input": ev.additional_input
    #     })
    #     
    #     user_feedback = ev.additional_input[0] if ev.additional_input else ""
    #     
    #     # ä½¿ç”¨planning judgeæ¥åˆ¤æ–­ç”¨æˆ·æ˜¯å¦æ¥å—è®¡åˆ’
    #     llm_input = self.planning_judge_formatter.format(query=user_feedback)
    #     
    #     print(f"âŒ›ï¸è¯·æ±‚{config.PLANNING_MODEL_NAME}æ¨¡å‹åˆ¤æ–­ç”¨æˆ·åé¦ˆ...")
    #     response_gen = await self.planning_judge_llm.astream_chat(messages=llm_input)
    #     
    #     async for response in response_gen:
    #         if hasattr(response, 'delta') and response.delta:
    #             print(response.delta, end='', flush=True)
    #     
    #     judge_result = response.message.content.strip()
    #     
    #     if "è‚¯å®š" in judge_result or "å¼€å§‹" in user_feedback or "æ˜¯" in user_feedback:
    #         # ç”¨æˆ·æ¥å—è®¡åˆ’ï¼Œå¼€å§‹æ‰§è¡Œ
    #         await self.workflow_strategy.on_step_complete("handle_user_feedback", {
    #             "decision": "accepted",
    #             "user_feedback": user_feedback
    #         })
    #         return PrepEvent()
    #     else:
    #         # ç”¨æˆ·è¦æ±‚ä¿®æ”¹è®¡åˆ’ï¼Œé‡æ–°ç”Ÿæˆ
    #         await self.workflow_strategy.on_step_complete("handle_user_feedback", {
    #             "decision": "rejected",
    #             "user_feedback": user_feedback
    #         })
    #         return PlanningEvent(input=ev.input, additional_input=ev.additional_input)

    # @step
    # async def fetch_usr_feedback(
    #     self, ctx: Context, ev: InputRequiredEvent
    # ) -> HumanResponseEvent:
    #     usr_feedback = input(">" + ev.prefix)
    #     return HumanResponseEvent(response=usr_feedback)

    # @step
    # async def send_usr_feedback(
    #     self, ctx: Context, ev: HumanResponseEvent
    # ) -> ConciergeEvent:
    #     memory = await ctx.store.get("memory", default=None)
    #     query = memory.get("usr_msg", default=None)
    #     return ConciergeEvent(input=query, additional_input=[ev.response])

    @step
    async def prepare_chat_history(
            self, ctx: Context, ev: PrepEvent
    ) -> InputEvent:
        # get chat history
        memory = await ctx.store.get("memory")
        chat_history = memory.get()
        current_reasoning = await ctx.store.get("current_reasoning", default=[])
        current_plan = await ctx.store.get("current_plan", default="")
        
        # ä»ä¸Šä¸‹æ–‡è·å–å½“å‰å·¥ä½œæµç¨‹åˆ†ç±»
        current_workflow_category = await ctx.store.get("current_workflow_category", default=None)

        # å¦‚æœcurrent_planä¸ºç©ºï¼Œè¯´æ˜è¿˜æ²¡æœ‰ç”Ÿæˆè®¡åˆ’ï¼Œç›´æ¥è¿›å…¥æ‰§è¡Œé˜¶æ®µ
        if not current_plan.strip():
            print("âš ï¸ å½“å‰æ²¡æœ‰æ‰§è¡Œè®¡åˆ’ï¼Œç›´æ¥è¿›å…¥æ‰§è¡Œé˜¶æ®µ")
            # format the prompt with react instructions
            # ä¼ é€’ä¸Šä¸‹æ–‡å‚æ•°ç»™formatterï¼ŒåŒ…æ‹¬category
            llm_input = await self.format_with_mcp_tools(chat_history, current_reasoning, current_plan,
                                                    await self.get_mcp_tool_descriptions(current_workflow_category), 
                                                    ctx, current_workflow_category)

            return InputEvent(input=llm_input)

        if current_reasoning:
            # call llm to update plan
            update_llm_input = self.plan_update_formatter.format(
                current_plan=current_plan,
                current_reasoning=current_reasoning,
                chat_history=chat_history,
            )

            print(f"âŒ›ï¸è¯·æ±‚{config.PLANNING_MODEL_NAME}æ¨¡å‹å“åº”...")
            response_gen = await self.plan_update_llm.astream_chat(
                messages=update_llm_input,
            )

            # å®šä¹‰å¼‚æ­¥å›è°ƒå‡½æ•°
            async def on_planning_thinking(content, metadata):
                # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
                current_category = await ctx.store.get("current_workflow_category", default=None)
                thinking_metadata = {"step": "è®¡åˆ’æ›´æ–°æ€è€ƒ"}
                if current_category:
                    thinking_metadata["category"] = current_category
                await self.workflow_strategy.on_streaming_content(
                    content, "planning", "thinking", thinking_metadata
                )

            async def on_planning_content(content, metadata):
                # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
                current_category = await ctx.store.get("current_workflow_category", default=None)
                content_metadata = {"step": "è®¡åˆ’æ›´æ–°è¾“å‡º"}
                if current_category:
                    content_metadata["category"] = current_category
                await self.workflow_strategy.on_streaming_content(
                    content, "planning", "output", content_metadata
                )

            # ä½¿ç”¨StreamingResponseParserè§£ææµå¼å“åº”
            response_content = await self.response_parser.parse_streaming_response(
                response_gen,
                on_thinking=on_planning_thinking,
                on_content=on_planning_content,
                thinking_metadata={"step": "è®¡åˆ’æ›´æ–°æ€è€ƒ", "phase": "planning"},
                content_metadata={"step": "è®¡åˆ’æ›´æ–°è¾“å‡º", "phase": "planning"}
            )

            generated_plan = self.response_parser.extract_final_content(response_content)

            # Extract the plan from the complete response
            if generated_plan.strip():
                current_plan = generated_plan
            # update the plan according to current_reasoning steps
            print("âœ…Update plan.md")
            write_plans_to_md("./plan.md", current_plan)  # write the plan to a markdown file

        print("\nâ¬‡ï¸Current plan: \n\n", current_plan)
        # format the prompt with react instructions
        tool_descriptions = await self.get_mcp_tool_descriptions(current_workflow_category)
        # ç”±äºformatteræœŸæœ›BaseToolåˆ—è¡¨ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„formatè°ƒç”¨
        llm_input = await self.format_with_mcp_tools(chat_history, current_reasoning, current_plan, tool_descriptions, ctx, current_workflow_category)

        return InputEvent(input=llm_input)

    @step
    async def handle_llm_input(
            self, ctx: Context, ev: InputEvent
    ) -> ToolCallEvent | StopEvent:
        await self.workflow_strategy.on_step_start("llm_reasoning", {
            "input_length": len(ev.input)
        })
        chat_history = ev.input
        # print("\n" + "=" * 20 + "chat history" + "=" * 20 + "\n")
        # print(chat_history)
        current_reasoning = await ctx.store.get("current_reasoning", default=[])
        memory = await ctx.store.get("memory")

        print(f"âŒ›ï¸è¯·æ±‚{config.DEFAULT_MODEL_NAME}æ¨¡å‹å“åº”...")
        
        # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
        current_category = await ctx.store.get("current_workflow_category", default=None)
        event_metadata = {
            "step": "llm_request"
        }
        if current_category:
            event_metadata["category"] = current_category
            
        await self.workflow_strategy.on_workflow_event("llm_request", "æ­£åœ¨å‘æ¨¡å‹å‘é€æ¨ç†è¯·æ±‚...", event_metadata)
        
        response_gen = await self.llm.astream_chat(chat_history)
        
        # å®šä¹‰å¼‚æ­¥å›è°ƒå‡½æ•°
        async def on_llm_thinking(content, metadata):
            # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
            current_category = await ctx.store.get("current_workflow_category", default=None)
            thinking_metadata = {"step": "LLMæ¨ç†æ€è€ƒ"}
            if current_category:
                thinking_metadata["category"] = current_category
            await self.workflow_strategy.on_streaming_content(
                content, "llm", "thinking", thinking_metadata
            )
        
        async def on_llm_content(content, metadata):
            # è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
            current_category = await ctx.store.get("current_workflow_category", default=None)
            content_metadata = {"step": "LLMæ¨ç†å›å¤"}
            if current_category:
                content_metadata["category"] = current_category
            await self.workflow_strategy.on_streaming_content(
                content, "llm", "output", content_metadata
            )
        
        # ä½¿ç”¨StreamingResponseParserè§£ææµå¼å“åº”
        response_content = await self.response_parser.parse_streaming_response(
            response_gen,
            on_thinking=on_llm_thinking,
            on_content=on_llm_content,
            thinking_metadata={"step": "LLMæ¨ç†æ€è€ƒ", "phase": "reasoning"},
            content_metadata={"step": "LLMæ¨ç†å›å¤", "phase": "reasoning"}
        )
        
        try:
            # ä»å“åº”ä¸­æå–ç­”æ¡ˆä¸»ä½“
            answer_body = self.response_parser.extract_final_content(response_content)

            reasoning_step = self.output_parser.parse(answer_body)
            current_reasoning.append(reasoning_step)
            print("\n" + "=" * 20 + "ReActå‚æ•°" + "=" * 20 + "\n")
            print(reasoning_step.get_content())

            if reasoning_step.is_done:
                memory.put(
                    ChatMessage(
                        role="assistant", content=reasoning_step.response
                    )
                )
                await ctx.store.set("memory", memory)
                await ctx.store.set("current_reasoning", current_reasoning)

                sources = await ctx.store.get("sources", default=[])

                return StopEvent(
                    result={
                        "response": reasoning_step.response,
                        "sources": [sources],
                        "reasoning": current_reasoning,
                    }
                )
            elif isinstance(reasoning_step, ActionReasoningStep):
                tool_name = reasoning_step.action
                tool_args = reasoning_step.action_input
                return ToolCallEvent(
                    tool_calls=[
                        ToolSelection(
                            tool_id="fake",
                            tool_name=tool_name,
                            tool_kwargs=tool_args,
                        )
                    ]
                )
            elif isinstance(reasoning_step, MilestoneReasoningStep):
                await ctx.store.set("current_reasoning", current_reasoning)
                return PrepEvent()
        except Exception as e:
            current_reasoning.append(
                ObservationReasoningStep(
                    observation=f"There was an error in parsing my reasoning: {e}"
                )
            )
            await ctx.store.set("current_reasoning", current_reasoning)

        # if no tool calls or final response, iterate again
        return PrepEvent()

    @step
    async def handle_tool_calls(
            self, ctx: Context, ev: ToolCallEvent
    ) -> PrepEvent:
        tool_calls = ev.tool_calls
        current_reasoning = await ctx.store.get("current_reasoning", default=[])
        sources = await ctx.store.get("sources", default=[])

        # ç¡®ä¿MCPå®¢æˆ·ç«¯å·²è¿æ¥
        await self.mcp_client.ensure_connected()

        # é€šè¿‡MCPè°ƒç”¨å·¥å…·
        for tool_call in tool_calls:
            # å¯¹æ€»ç»“å·¥å…·æå‰æ³¨å…¥å‚æ•°ï¼Œé¿å…ç”±å¤§æ¨¡å‹æ„é€ å¤§å…¥å‚
            if tool_call.tool_name == "conclude_document_chunks":
                try:
                    cached_chunks = await ctx.store.get("relevant_doc_chunks", default=[])
                except Exception:
                    cached_chunks = []

                conclude_kwargs = dict(tool_call.tool_kwargs or {})
                if "query" not in conclude_kwargs:
                    memory = await ctx.store.get("memory")
                    user_msgs = [m for m in memory.get() if getattr(m, "role", "") == "user"]
                    conclude_kwargs["query"] = user_msgs[-1].content if user_msgs else ""
                
                # æ³¨å…¥doc_chunkså‚æ•°
                if cached_chunks:
                    conclude_kwargs["doc_chunks"] = cached_chunks

                # å›å†™åˆ°å½“å‰å·¥å…·è°ƒç”¨å‚æ•°ï¼Œä»¥ä¾¿åç»­èµ°ç»Ÿä¸€è°ƒç”¨æµç¨‹
                tool_call.tool_kwargs = conclude_kwargs
            
            # ä¸ºæ–‡æ¡£æ£€ç´¢å·¥å…·æ³¨å…¥categoryå‚æ•°
            if tool_call.tool_name == "search_documents":
                tool_kwargs = dict(tool_call.tool_kwargs or {})
                if "category" not in tool_kwargs:
                    # ä»ä¸Šä¸‹æ–‡è·å–å½“å‰å·¥ä½œæµåˆ†ç±»
                    try:
                        current_workflow_category = await ctx.store.get(
                            "current_workflow_category", default="research-general"
                        )
                        tool_kwargs["category"] = current_workflow_category
                    except Exception:
                        # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        tool_kwargs["category"] = "research-general"

                # å›å†™åˆ°å½“å‰å·¥å…·è°ƒç”¨å‚æ•°
                tool_call.tool_kwargs = tool_kwargs

            # å‘é€å·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶
            current_workflow_category = await ctx.store.get("current_workflow_category", default="research-general")
            print(f"ğŸŸ¢ å‘é€tool_call_startäº‹ä»¶: tool={tool_call.tool_name}, category={current_workflow_category}")
            await self.workflow_strategy.on_tool_call_start(tool_call.tool_name, tool_call.tool_kwargs, current_workflow_category)



            # ä½¿ç”¨MCPè°ƒç”¨å·¥å…·
            result = await self.mcp_client.call_tool(
                tool_call.tool_name,
                tool_call.tool_kwargs
            )
            print(tool_call.tool_name + "å·¥å…·è¿”å›ç»“æœï¼š" + str(result))

            # å‘é€å·¥å…·è°ƒç”¨å®Œæˆäº‹ä»¶
            print(f"ğŸŸ¢ å‘é€tool_call_completeäº‹ä»¶: tool={tool_call.tool_name}, category={current_workflow_category}")
            await self.workflow_strategy.on_tool_call_complete(tool_call.tool_name, result, current_workflow_category)
            # å¤„ç†MCPè¿”å›ç»“æœ
            if isinstance(result, dict) and "result" in result:
                tool_output_text = result["result"]
            else:
                tool_output_text = str(result)

            # ç‰¹æ®Šå¤„ç†ï¼šæ–‡æ¡£æ£€ç´¢ç±»å·¥å…·
            if tool_call.tool_name == "search_documents":
                # ä»å­—ç¬¦ä¸²ä¸­æå–æ‰€æœ‰è¢«åŒ…è£¹çš„ JSON æ–‡æœ¬
                json_strings = re.findall(
                    r"TextContent\(type='text',\s*text='((?:\\'|[^'])*)',\s*annotations=None(?:,\s*meta=None)?\)",
                    tool_output_text,
                    re.DOTALL,
                )

                doc_chunks: List[str] = []
                parsed = None
                if json_strings:
                    try:
                        raw_text = json_strings[0]
                        safe_text = raw_text.replace('\\', '\\\\')
                        safe_text = safe_text.replace('\\\\n', '\\n')
                        safe_text = safe_text.replace('\\\\t', '\\t')
                        safe_text = safe_text.replace('\\\\r', '\\r')
                        safe_text = safe_text.replace('\\\\"', '\\"')
                        parsed = json.loads(safe_text)
                    except Exception:
                        parsed = None
                else:
                    try:
                        parsed = json.loads(tool_output_text)
                    except Exception:
                        try:
                            import ast
                            parsed = ast.literal_eval(tool_output_text)
                        except Exception:
                            parsed = None

                if isinstance(parsed, list):
                    for item in parsed:
                        if isinstance(item, dict):
                            chunks = item.get("chunk", [])
                            if isinstance(chunks, list):
                                doc_chunks.extend([c for c in chunks if isinstance(c, str)])
                            elif isinstance(chunks, str):
                                doc_chunks.append(chunks)
                elif isinstance(parsed, dict):
                    chunks = parsed.get("chunk", [])
                    if isinstance(chunks, list):
                        doc_chunks.extend([c for c in chunks if isinstance(c, str)])
                    elif isinstance(chunks, str):
                        doc_chunks.append(chunks)
                elif json_strings:
                    doc_chunks = json_strings

                # å¦‚æœæœ‰æŸ¥è¯¢ä¸”å¯ç”¨äº†è¿‡æ»¤å™¨ï¼Œåˆ™å¹¶è¡Œè¿‡æ»¤ç›¸å…³ chunk
                query = tool_call.tool_kwargs.get("query")
                current_workflow_category = await ctx.store.get("current_workflow_category", default="research-general")
                if self.filter_llm and query:
                    relevant_chunks = await self._filter_chunks_parallel(doc_chunks, query, current_workflow_category)
                else:
                    relevant_chunks = doc_chunks

                # æ›´æ–°ä¸Šä¸‹æ–‡
                sources.append(relevant_chunks)
                current_reasoning.append(ObservationReasoningStep(observation=str(relevant_chunks)))
                # å°†ç›¸å…³æ–‡æ¡£å—ç¼“å­˜åˆ°ä¸Šä¸‹æ–‡ï¼Œä¾›åç»­æ€»ç»“å·¥å…·ä½¿ç”¨
                try:
                    await ctx.store.set("relevant_doc_chunks", relevant_chunks)
                except Exception:
                    pass
            else:
                # é€šç”¨å¤„ç†ï¼šå…¶ä»–å·¥å…·ç›´æ¥ä¿å­˜è¾“å‡º
                sources.append(tool_output_text)
                current_reasoning.append(
                    ObservationReasoningStep(observation=str(tool_output_text))
                )



        # ä¿å­˜æ›´æ–°åçš„çŠ¶æ€åˆ°ä¸Šä¸‹æ–‡
        await ctx.store.set("sources", sources)
        await ctx.store.set("current_reasoning", current_reasoning)

        # å‡†å¤‡ä¸‹ä¸€è½®è¿­ä»£
        return PrepEvent()
    # @step
    # async def conclude_doc_chunks(
    #         self, ctx: Context, ev: ConcludeEvent
    # ) -> PrepEvent:
    #     memory = await ctx.store.get("memory", default=None)
    #     query = await ctx.store.get("rag_input", default=None)
    #     current_reasoning = await ctx.store.get("current_reasoning", default=[])
    #     print("\n" + "=" * 20 + "ç­›é€‰ådocå†…å®¹" + "=" * 20 + "\n")
    #     print(ev.input)
    #
    #     conclusion = ""  # æ¯ä¸ªchunkçš„æ€»ç»“ï¼Œä¼šè¢«å•ç‹¬appendåˆ°conclusionä¸­ã€‚ç›®å‰æš‚æ—¶ä¸è€ƒè™‘è·¨chunkçš„å…³è”å…³ç³»
    #
    #     for chunk in ev.input:
    #         llm_input = self.conclusion_formatter.format(query=query, doc_chunks=chunk)
    #
    #         response_gen = await self.conclusion_llm.astream_chat(llm_input)
    #
    #         # å®šä¹‰å¼‚æ­¥å›è°ƒå‡½æ•°
    #         async def on_conclusion_thinking(content, metadata):
    #             await self.workflow_strategy.on_streaming_content(
    #                 content, "conclusion_thinking", {"step": "æ–‡æ¡£æ€»ç»“æ€è€ƒ"}
    #             )
    #
    #         async def on_conclusion_content(content, metadata):
    #             await self.workflow_strategy.on_streaming_content(
    #                 content, "conclusion_output", {"step": "æ–‡æ¡£æ€»ç»“è¾“å‡º"}
    #             )
    #
    #         # ä½¿ç”¨StreamingResponseParserè§£ææµå¼å“åº”
    #         response_content = await self.response_parser.parse_streaming_response(
    #             response_gen,
    #             on_thinking=on_conclusion_thinking,
    #             on_content=on_conclusion_content,
    #             thinking_metadata={"step": "æ–‡æ¡£æ€»ç»“æ€è€ƒ", "phase": "conclusion"},
    #             content_metadata={"step": "æ–‡æ¡£æ€»ç»“è¾“å‡º", "phase": "conclusion"}
    #         )
    #
    #         # æå–æœ€ç»ˆå†…å®¹
    #         final_content = self.response_parser.extract_final_content(response_content)
    #         conclusion += final_content + "\n\n"
    #         # print("\n" + "=" * 20 + "llmè¾“å‡º" + "=" * 20 + "\n")
    #         # print(response.message.content)
    #
    #     # put the llm conclusion to memory
    #     memory.put(
    #         ChatMessage(
    #             role="assistant", content=conclusion
    #         )
    #     )
    #
    #     current_reasoning.append(
    #         ObservationReasoningStep(observation=conclusion)
    #     )
    #
    #     await ctx.store.set("memory", memory)
    #     await ctx.store.set("current_reasoning", current_reasoning)
    #
    #     return PrepEvent()

    async def _on_parallel_thinking(self, content: str, metadata: dict):
        """å¹¶è¡Œå·¥ä½œæµç¨‹æ€è€ƒè¿‡ç¨‹å›è°ƒ"""
        category = metadata.get("category", "æœªçŸ¥åˆ†ç±»")
        await self.workflow_strategy.on_streaming_content(
            content, 
            "planning", 
            "thinking",
            {
                "step": f"[{category}] å¹¶è¡Œæ€è€ƒè¿‡ç¨‹",
                "category": category,
                "phase": "parallel_execution"
            }
        )

    async def _on_parallel_content(self, content: str, metadata: dict):
        """å¹¶è¡Œå·¥ä½œæµç¨‹å†…å®¹å›è°ƒ"""
        category = metadata.get("category", "æœªçŸ¥åˆ†ç±»")
        await self.workflow_strategy.on_streaming_content(
            content, 
            "planning", 
            "output",
            {
                "step": f"[{category}] å¹¶è¡Œæ‰§è¡Œè¾“å‡º",
                "category": category,
                "phase": "parallel_execution"
            }
        )
